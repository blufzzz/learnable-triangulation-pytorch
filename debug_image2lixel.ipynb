{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image, to_numpy \n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose, display_pose, CONNECTIVITY_DICT\n",
    "from mvn.utils.op import get_coord_volumes, integrate_tensor_3d_with_coordinates, unproject_heatmaps\n",
    "from mvn.utils import cfg\n",
    "\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "\n",
    "from train_decomposition import setup_human36m_dataloaders\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "retval = {\n",
    "    'subject_names': ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11'],\n",
    "    'camera_names': ['54138969', '55011271', '58860488', '60457274'],\n",
    "    'action_names': [\n",
    "        'Directions-1', 'Directions-2',\n",
    "        'Discussion-1', 'Discussion-2',\n",
    "        'Eating-1', 'Eating-2',\n",
    "        'Greeting-1', 'Greeting-2',\n",
    "        'Phoning-1', 'Phoning-2',\n",
    "        'Posing-1', 'Posing-2',\n",
    "        'Purchases-1', 'Purchases-2',\n",
    "        'Sitting-1', 'Sitting-2',\n",
    "        'SittingDown-1', 'SittingDown-2',\n",
    "        'Smoking-1', 'Smoking-2',\n",
    "        'TakingPhoto-1', 'TakingPhoto-2',\n",
    "        'Waiting-1', 'Waiting-2',\n",
    "        'Walking-1', 'Walking-2',\n",
    "        'WalkingDog-1', 'WalkingDog-2',\n",
    "        'WalkingTogether-1', 'WalkingTogether-2']\n",
    "}\n",
    "\n",
    "device = 'cuda:0' #torch.cuda.current_device()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "def to_numpy(cuda_tensor):\n",
    "    return cuda_tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nn.Conv3d(64,64,kernel_size=[1,2,2],stride=[1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.optim.Adam(c.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32, 16, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,64,32,32,32)\n",
    "c(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(x, torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum((3,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchgeometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/human36m/train/human36m_vol_image2lixel.yaml'\n",
    "config = cfg.load_config(config_path)\n",
    "num_joints = config.model.backbone.num_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from: ./data/pose_resnet_4.5_pixels_human36m.pth\n",
      "Successfully loaded pretrained weights for backbone\n"
     ]
    }
   ],
   "source": [
    "from mvn.models.image2lixel import I2LModel\n",
    "model = I2LModel(config, device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, _ = setup_human36m_dataloaders(config, is_train=True, distributed_train=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "(images_batch, \n",
    "keypoints_3d_gt, \n",
    "keypoints_3d_validity_gt, \n",
    "proj_matricies_batch) = dataset_utils.prepare_batch(batch, device)\n",
    "\n",
    "heatmaps_pred, keypoints_2d_pred, cuboids_pred, base_points_pred = None, None, None, None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/nfs/hpc2_storage/ibulygin/learnable-triangulation-pytorch/mvn/models/image2lixel.py\u001b[0m(45)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     43 \u001b[0;31m        \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_img_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 45 \u001b[0;31m        \u001b[0mjoint_coord_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose_img_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     46 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mjoint_coord_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> pose_img_feat.shape\n",
      "torch.Size([1, 2048, 8, 8])\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "out = model(images_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare backbones with volumetric baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
