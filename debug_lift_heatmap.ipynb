{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MSingleViewDataset\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image,to_numpy\n",
    "from mvn.models.triangulation import VolumetricTriangulationNet\n",
    "from mvn.models.volumetric_temporal import VolumetricTemporalNet\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "from mvn.utils import img\n",
    "from mvn.utils import multiview\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils import op\n",
    "from mvn.utils import vis\n",
    "from mvn.utils import misc\n",
    "from mvn.utils import cfg\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MSingleViewDataset\n",
    "\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "from mvn.models.temporal import Seq2VecRNN,\\\n",
    "                                FeaturesAR_RNN,\\\n",
    "                                FeaturesAR_CNN1D,\\\n",
    "                                FeaturesAR_CNN2D_UNet,\\\n",
    "                                FeaturesAR_CNN2D_ResNet\n",
    "\n",
    "from mvn.models.volumetric_temporal import VolumetricTemporalNet,\\\n",
    "                                           VolumetricTemporalAdaINNet,\\\n",
    "                                           VolumetricFRAdaINNet\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "retval = {\n",
    "    'subject_names': ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11'],\n",
    "    'camera_names': ['54138969', '55011271', '58860488', '60457274'],\n",
    "    'action_names': [\n",
    "        'Directions-1', 'Directions-2',\n",
    "        'Discussion-1', 'Discussion-2',\n",
    "        'Eating-1', 'Eating-2',\n",
    "        'Greeting-1', 'Greeting-2',\n",
    "        'Phoning-1', 'Phoning-2',\n",
    "        'Posing-1', 'Posing-2',\n",
    "        'Purchases-1', 'Purchases-2',\n",
    "        'Sitting-1', 'Sitting-2',\n",
    "        'SittingDown-1', 'SittingDown-2',\n",
    "        'Smoking-1', 'Smoking-2',\n",
    "        'TakingPhoto-1', 'TakingPhoto-2',\n",
    "        'Waiting-1', 'Waiting-2',\n",
    "        'Walking-1', 'Walking-2',\n",
    "        'WalkingDog-1', 'WalkingDog-2',\n",
    "        'WalkingTogether-1', 'WalkingTogether-2']\n",
    "}\n",
    "\n",
    "CHANNELS_LIST = [16, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 128, 128, 128, 128, 128,\\\n",
    "                                  128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\\\n",
    "                                  128, 128, 128, 128, 128, 64, 64, 64, 32, 32, 32, 32, 32]\n",
    "\n",
    "device = 'cuda:0' #torch.cuda.current_device()\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/human36m/train/human36m_vol_temporal_adain.yaml'\n",
    "config = cfg.load_config(config_path)\n",
    "config.model.volume_features_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from: ./data/pose_resnet_4.5_pixels_human36m.pth\n",
      "Parameters [{'final_layer.weight', 'final_layer.bias'}] were not inited\n",
      "Successfully loaded pretrained weights for backbone\n"
     ]
    }
   ],
   "source": [
    "self = {\n",
    "    \"vol\": VolumetricTriangulationNet,\n",
    "    \"vol_temporal\": VolumetricTemporalNet,\n",
    "    \"vol_temporal_adain\":VolumetricTemporalAdaINNet,\n",
    "    \"vol_temporal_fr_adain\":VolumetricFRAdaINNet,\n",
    "    \"vol_temporal_lstm_v2v\":VolumetricTemporalNet\n",
    "}[\"vol_temporal_adain\"](config, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, _ = setup_human36m_dataloaders(config, is_train=True, distributed_train=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    images_batch, keypoints_3d_gt, keypoints_3d_validity_gt, proj_matricies_batch = dataset_utils.prepare_batch(batch, device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, dt = images_batch.shape[:2]\n",
    "image_shape = images_batch.shape[-2:]\n",
    "images_batch = images_batch.view(-1, 3, *image_shape)\n",
    "\n",
    "# forward backbone\n",
    "heatmaps, features, alg_confidences, vol_confidences, bottleneck = self.backbone(images_batch)\n",
    "\n",
    "# reshape back and take only last view (pivot)\n",
    "images_batch = images_batch.view(batch_size, dt, 3, *image_shape)[:,-1,...].unsqueeze(1)\n",
    "\n",
    "# calcualte shapes\n",
    "features_shape = features.shape[-2:]\n",
    "features_channels = features.shape[1]\n",
    "\n",
    "# change camera intrinsics\n",
    "new_cameras = deepcopy(batch['cameras'])\n",
    "for view_i in range(dt):\n",
    "    for batch_i in range(batch_size):\n",
    "        new_cameras[view_i][batch_i].update_after_resize(image_shape, features_shape)\n",
    "\n",
    "proj_matricies_batch = torch.stack([torch.stack([torch.from_numpy(camera.projection) \\\n",
    "                                    for camera in camera_batch], dim=0) \\\n",
    "                                    for camera_batch in new_cameras], dim=0).transpose(1, 0)  # shape (batch_size, dt, 3, 4)\n",
    "\n",
    "proj_matricies_batch = proj_matricies_batch.float().to(device)\n",
    "proj_matricies_batch = proj_matricies_batch[:,-1,...].unsqueeze(1) \n",
    "\n",
    "features = features.view(batch_size, dt, features_channels, *features_shape)\n",
    "pivot_features = features[:,-1,...]\n",
    "style_features = features if self.include_pivot else features[:,:-1,...].contiguous() \n",
    "pivot_features = self.process_features(pivot_features).unsqueeze(1)\n",
    "\n",
    "if self.encoder_type == 'backbone':\n",
    "\n",
    "    bottleneck_shape = bottleneck.shape[-2:]\n",
    "    bottleneck_channels = bottleneck.shape[1]\n",
    "\n",
    "    bottleneck = bottleneck.view(batch_size, dt, bottleneck_channels, *bottleneck_shape)\n",
    "    if not self.include_pivot:\n",
    "        bottleneck = bottleneck[:,:-1,...].contiguous()\n",
    "    bottleneck = bottleneck.view(-1, # batch_size*(dt-1)\n",
    "                                 bottleneck_channels,\n",
    "                                *bottleneck_shape)\n",
    "\n",
    "    if not self.style_grad_for_backbone:\n",
    "        bottleneck = bottleneck.detach()\n",
    "\n",
    "    encoded_features = self.encoder(bottleneck)\n",
    "else:\n",
    "    style_features = style_features.view(-1, # batch_size*(dt-1)\n",
    "                                         features_channels,\n",
    "                                        *features_shape)\n",
    "    if self.style_grad_for_backbone:\n",
    "        style_features = style_features.detach()\n",
    "    encoded_features = self.encoder(style_features)\n",
    "\n",
    "encoded_features = encoded_features.view(batch_size,\n",
    "                                         -1, # (dt-1) \n",
    "                                         self.encoded_feature_space)\n",
    "\n",
    "style_vector = self.features_sequence_to_vector(encoded_features, device=device) # [batch_size, 512]\n",
    "\n",
    "if self.use_precalculated_pelvis:\n",
    "    tri_keypoints_3d = torch.from_numpy(np.array(batch['pred_keypoints_3d'])).type(torch.float).to(device)\n",
    "\n",
    "elif self.use_gt_pelvis:\n",
    "    tri_keypoints_3d = torch.from_numpy(np.array(batch['keypoints_3d'])).type(torch.float).to(device)\n",
    "\n",
    "else:\n",
    "    raise RuntimeError('In absence of precalculated pelvis or gt pelvis, self.use_volumetric_pelvis should be True') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/nfs/hpc2_storage/ibulygin/learnable-triangulation-pytorch/mvn/utils/op.py\u001b[0m(228)\u001b[0;36munproject_heatmaps\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    226 \u001b[0;31m            \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    227 \u001b[0;31m            \u001b[0;31m# prepare to F.grid_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 228 \u001b[0;31m            \u001b[0mgrid_coord_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    229 \u001b[0;31m            \u001b[0mcurrent_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    230 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> grid_coord_proj.shape\n",
      "torch.Size([262144, 2])\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-66390908cdae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mcoord_volumes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0mvolume_aggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume_aggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                 \u001b[0mvol_confidences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvol_confidences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                 )\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvolumes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/hpc2_storage/ibulygin/learnable-triangulation-pytorch/mvn/utils/op.py\u001b[0m in \u001b[0;36munproject_heatmaps\u001b[0;34m(heatmaps, proj_matricies, coord_volumes, volume_aggregation_method, vol_confidences, volumes_multipliers)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m# prepare to F.grid_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mgrid_coord_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mcurrent_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# zero out non-valid points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/hpc2_storage/ibulygin/learnable-triangulation-pytorch/mvn/utils/op.py\u001b[0m in \u001b[0;36munproject_heatmaps\u001b[0;34m(heatmaps, proj_matricies, coord_volumes, volume_aggregation_method, vol_confidences, volumes_multipliers)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m# prepare to F.grid_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mgrid_coord_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mcurrent_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_coord_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# zero out non-valid points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hpc2_storage/ibulygin/miniconda3/envs/mvn/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hpc2_storage/ibulygin/miniconda3/envs/mvn/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# amend coord_volumes position                                                         \n",
    "coord_volumes, cuboids, base_points = op.get_coord_volumes(self.kind, \n",
    "                                                        self.training, \n",
    "                                                        self.rotation,\n",
    "                                                        self.cuboid_side,\n",
    "                                                        self.volume_size, \n",
    "                                                        device,\n",
    "                                                        keypoints=tri_keypoints_3d\n",
    "                                                        )\n",
    "\n",
    "# lift each featuremap to distinct volume and aggregate \n",
    "volumes = op.unproject_heatmaps(pivot_features,  \n",
    "                                proj_matricies_batch, \n",
    "                                coord_volumes, \n",
    "                                volume_aggregation_method=self.volume_aggregation_method,\n",
    "                                vol_confidences=vol_confidences\n",
    "                                )\n",
    "print (volumes[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
