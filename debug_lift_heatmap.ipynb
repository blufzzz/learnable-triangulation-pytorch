{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from mvn.utils import multiview\n",
    "\n",
    "from mvn.datasets.cmu import CMUSceneDataset\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MSingleViewDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image\n",
    "from mvn.models.temporal import TemporalModel, TemporalModelBase\n",
    "from mvn.models.basic import VolumetricTriangulationNet, VolumetricTemporalTriangulationNet\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "\n",
    "from mvn.utils import img\n",
    "from mvn.utils import multiview\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils import op\n",
    "from mvn.utils import vis\n",
    "from mvn.utils import misc\n",
    "from mvn.utils import cfg\n",
    "\n",
    "from time import time\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "from mvn.datasets import cmu, kth\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MSingleViewDataset\n",
    "\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "retval = {\n",
    "    'subject_names': ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11'],\n",
    "    'camera_names': ['54138969', '55011271', '58860488', '60457274'],\n",
    "    'action_names': [\n",
    "        'Directions-1', 'Directions-2',\n",
    "        'Discussion-1', 'Discussion-2',\n",
    "        'Eating-1', 'Eating-2',\n",
    "        'Greeting-1', 'Greeting-2',\n",
    "        'Phoning-1', 'Phoning-2',\n",
    "        'Posing-1', 'Posing-2',\n",
    "        'Purchases-1', 'Purchases-2',\n",
    "        'Sitting-1', 'Sitting-2',\n",
    "        'SittingDown-1', 'SittingDown-2',\n",
    "        'Smoking-1', 'Smoking-2',\n",
    "        'TakingPhoto-1', 'TakingPhoto-2',\n",
    "        'Waiting-1', 'Waiting-2',\n",
    "        'Walking-1', 'Walking-2',\n",
    "        'WalkingDog-1', 'WalkingDog-2',\n",
    "        'WalkingTogether-1', 'WalkingTogether-2']\n",
    "}\n",
    "# import nonechucks\n",
    "\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_root = '../../logs/vanilla_v2v/h36_sv_dist_default_bs2-5@10.10.2019-17:58:03'\n",
    "# config_path = experiment_root + '/config.yaml'\n",
    "\n",
    "config_path = './experiments/paper/human36m/vol_2stage2/human36m_v2v_softmax.yaml'\n",
    "\n",
    "config = cfg.load_config(config_path)\n",
    "\n",
    "# config.opt.val_batch_size=1\n",
    "# config.dataset.val.retain_every_n_frames_in_test = 1\n",
    "\n",
    "train_loader, val_loader = setup_human36m_dataloaders(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from: /media/hpc2_storage/ibulygin/multi-view-net-files/pose_net/pose_resnet_4.5_pixels_human36m.pth\n",
      "Reiniting first 16/17 filters: module.final_layer.weight\n",
      "Reiniting first 16 values in: module.final_layer.bias\n",
      "Successfully loaded pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = VolumetricTemporalTriangulationNet(config.model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch, keypoints_3d_batch_gt, keypoints_3d_validity_batch_gt, proj_matricies_batch = dataset_utils.prepare_batch(batch, device, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = batch['keypoints_3d']\n",
    "keypoints = torch.from_numpy(keypoints).to(device)\n",
    "volume_size = 64\n",
    "cuboid_side = 2500\n",
    "cuboids = []\n",
    "sides = np.array([cuboid_side, cuboid_side, cuboid_side])\n",
    "\n",
    "# keypoints = keypoints[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_volumes_batch = model.get_coord_volumes(cuboid_side, volume_size, device, keypoints)[0]\n",
    "coord_volumes_batch = coord_volumes_batch.view(-1, 64,64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_batch = torch.tensor(images_batch, dtype=torch.float, device=device)\n",
    "image_shape = images_batch.shape[-3:]\n",
    "bs,dt = images_batch.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps, features, tri_confidences, vol_confidences = model.backbone(images_batch.view(-1,*image_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.process_features(features)\n",
    "features = features.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 32, 96, 96])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_matricies_batch = proj_matricies_batch.view(-1, 1, *proj_matricies_batch.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_matricies_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64, 64, 64, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_volumes_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_volumes_batch.shape[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_heatmap_to_volume_2_old(heatmaps_batch, proj_matricies_batch, coord_volumes_batch, volume_aggregation_method='sum', vol_confidences=None, timestep_multiplier = 1.0):\n",
    "    device = heatmaps_batch.device\n",
    "    batch_size, n_views, n_joints, heatmap_shape = heatmaps_batch.shape[0], heatmaps_batch.shape[1], heatmaps_batch.shape[2], tuple(heatmaps_batch.shape[3:])\n",
    "    volume_shape = coord_volumes_batch.shape[1:4]\n",
    "    \n",
    "    # shape (bs, 32, 64, 64, 64)\n",
    "    volume_batch = torch.zeros(batch_size, n_joints, *volume_shape, device=device) if (volume_aggregation_method != 'no_aggregation') else []\n",
    "\n",
    "    # TODO: speed up this this ineffective loop\n",
    "    for batch_i in range(batch_size):\n",
    "        coord_volume = coord_volumes_batch[batch_i]\n",
    "        grid_coord = coord_volume.reshape((-1, 3))\n",
    "\n",
    "        volume_batch_to_aggregate = torch.zeros(n_views, n_joints, *volume_shape, device=device)\n",
    "\n",
    "        for view_i in range(n_views):\n",
    "            heatmap = heatmaps_batch[batch_i, view_i]\n",
    "            heatmap = heatmap.unsqueeze(0)\n",
    "\n",
    "            grid_coord_proj = multiview.project_3d_points_to_image_plane_without_distortion(\n",
    "                proj_matricies_batch[batch_i, view_i], grid_coord, convert_back_to_euclidean=False\n",
    "            )\n",
    "\n",
    "            invalid_mask = grid_coord_proj[:, 2] <= 0.0  # depth must be larger than 0.0\n",
    "\n",
    "            grid_coord_proj[grid_coord_proj[:, 2] == 0.0, 2] = 1.0  # not to divide by zero\n",
    "            grid_coord_proj = multiview.homogeneous_to_euclidean(grid_coord_proj)\n",
    "\n",
    "            # transform to [-1.0, 1.0] range\n",
    "            grid_coord_proj_transformed = torch.zeros_like(grid_coord_proj)\n",
    "            grid_coord_proj_transformed[:, 0] = 2 * (grid_coord_proj[:, 0] / heatmap_shape[0] - 0.5)\n",
    "            grid_coord_proj_transformed[:, 1] = 2 * (grid_coord_proj[:, 1] / heatmap_shape[1] - 0.5)\n",
    "            grid_coord_proj = grid_coord_proj_transformed\n",
    "\n",
    "            # NOTE: it's hack solution for now. In grid_proj appears inf values because of dividing by zero when converting from homogenious to euclidean\n",
    "            #grid_coord_proj = torch.clamp(grid_coord_proj, -1.01, 1.01)\n",
    "\n",
    "            # prepare to F.grid_sample\n",
    "            grid_coord_proj = grid_coord_proj.unsqueeze(1).unsqueeze(0)\n",
    "            current_volume = F.grid_sample(heatmap, grid_coord_proj)\n",
    "\n",
    "            # zero out non-valid points\n",
    "            current_volume = current_volume.view(n_joints, -1)\n",
    "            current_volume[:, invalid_mask] = 0.0\n",
    "\n",
    "            # reshape back to volume\n",
    "            current_volume = current_volume.view(n_joints, *volume_shape)\n",
    "\n",
    "            # collect\n",
    "            volume_batch_to_aggregate[view_i] = current_volume\n",
    "\n",
    "        # agregate resulting volume\n",
    "        if volume_aggregation_method.startswith('conf'):\n",
    "            volume_batch[batch_i] = (volume_batch_to_aggregate * vol_confidences[batch_i].view(n_views, n_joints, 1, 1, 1)).sum(0)\n",
    "        elif volume_aggregation_method == 'sum':\n",
    "            volume_batch[batch_i] = volume_batch_to_aggregate.sum(0)\n",
    "        elif volume_aggregation_method == 'max':\n",
    "            volume_batch[batch_i] = volume_batch_to_aggregate.max(0)[0]\n",
    "        elif volume_aggregation_method == 'softmax':\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate.clone()\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate_softmin.view(n_views, -1)\n",
    "            volume_batch_to_aggregate_softmin = nn.functional.softmax(volume_batch_to_aggregate_softmin, dim=0)\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate_softmin.view(n_views, n_joints, *volume_shape)\n",
    "\n",
    "            volume_batch[batch_i] = (volume_batch_to_aggregate * volume_batch_to_aggregate_softmin).sum(0)\n",
    "\n",
    "        elif volume_aggregation_method == 'no_aggregation':\n",
    "            volume_batch.append(volume_batch_to_aggregate) \n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown volume_aggregation_method: {}\".format(volume_aggregation_method))\n",
    "               \n",
    "    return volume_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 3, 4]),\n",
       " torch.Size([6, 1, 32, 96, 96]),\n",
       " torch.Size([6, 64, 64, 64, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_matricies_batch.shape, features.shape, coord_volumes_batch.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes  = lift_heatmap_to_volume_2_old(features,  \n",
    "                            proj_matricies_batch, \n",
    "                            coord_volumes_batch,\n",
    "                            volume_aggregation_method='no_aggregation',\n",
    "                            vol_confidences=vol_confidences,\n",
    "                            timestep_multiplier = model.timestep_multiplier\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64, 64, 64, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_volumes_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 32, 96, 96])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64, 64, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volumes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.timestep_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift_heatmap_to_volume_2(heatmaps_batch, proj_matricies_batch, coord_volumes_batch, volume_aggregation_method='sum', vol_confidences=None, timestep_multiplier = 1.0):\n",
    "    device = heatmaps_batch.device\n",
    "    batch_size, n_views, n_joints, heatmap_shape = heatmaps_batch.shape[0], heatmaps_batch.shape[1], heatmaps_batch.shape[2], tuple(heatmaps_batch.shape[3:])\n",
    "    volume_shape = coord_volumes_batch.shape[1:4]\n",
    "    \n",
    "    # shape (bs, 32, 64, 64, 64)\n",
    "    volume_batch = torch.zeros(batch_size, n_joints, *volume_shape, device=device) if (volume_aggregation_method != 'no_aggregation') else []\n",
    "\n",
    "    # TODO: speed up this this ineffective loop\n",
    "    for batch_i in range(batch_size):\n",
    "        coord_volume = coord_volumes_batch[batch_i]\n",
    "        grid_coord = coord_volume.reshape((-1, 3))\n",
    "\n",
    "        volume_batch_to_aggregate = torch.zeros(n_views, n_joints, *volume_shape, device=device)\n",
    "\n",
    "        for view_i in range(n_views):\n",
    "            heatmap = heatmaps_batch[batch_i, view_i]\n",
    "            heatmap = heatmap.unsqueeze(0)\n",
    "\n",
    "            grid_coord_proj = multiview.project_3d_points_to_image_plane_without_distortion(\n",
    "                proj_matricies_batch[batch_i, view_i], grid_coord, convert_back_to_euclidean=False\n",
    "            )\n",
    "\n",
    "            invalid_mask = grid_coord_proj[:, 2] <= 0.0  # depth must be larger than 0.0\n",
    "\n",
    "            grid_coord_proj[grid_coord_proj[:, 2] == 0.0, 2] = 1.0  # not to divide by zero\n",
    "            grid_coord_proj = multiview.homogeneous_to_euclidean(grid_coord_proj)\n",
    "\n",
    "            # transform to [-1.0, 1.0] range\n",
    "            grid_coord_proj_transformed = torch.zeros_like(grid_coord_proj)\n",
    "            grid_coord_proj_transformed[:, 0] = 2 * (grid_coord_proj[:, 0] / heatmap_shape[0] - 0.5)\n",
    "            grid_coord_proj_transformed[:, 1] = 2 * (grid_coord_proj[:, 1] / heatmap_shape[1] - 0.5)\n",
    "            grid_coord_proj = grid_coord_proj_transformed\n",
    "\n",
    "            # NOTE: it's hack solution for now. In grid_proj appears inf values because of dividing by zero when converting from homogenious to euclidean\n",
    "            #grid_coord_proj = torch.clamp(grid_coord_proj, -1.01, 1.01)\n",
    "\n",
    "            # prepare to F.grid_sample\n",
    "            grid_coord_proj = grid_coord_proj.unsqueeze(1).unsqueeze(0)\n",
    "            current_volume = F.grid_sample(heatmap, grid_coord_proj)\n",
    "\n",
    "            # zero out non-valid points\n",
    "            current_volume = current_volume.view(n_joints, -1)\n",
    "            current_volume[:, invalid_mask] = 0.0\n",
    "\n",
    "            # reshape back to volume\n",
    "            current_volume = current_volume.view(n_joints, *volume_shape)\n",
    "\n",
    "            # collect\n",
    "            volume_batch_to_aggregate[view_i] = current_volume\n",
    "\n",
    "        # agregate resulting volume\n",
    "        if volume_aggregation_method.startswith('conf'):\n",
    "            volume_batch[batch_i] = (volume_batch_to_aggregate * vol_confidences[batch_i].view(n_views, n_joints, 1, 1, 1)).sum(0)\n",
    "        elif volume_aggregation_method == 'sum':\n",
    "            volume_batch[batch_i] = volume_batch_to_aggregate.sum(0)\n",
    "        elif volume_aggregation_method == 'max':\n",
    "            volume_batch[batch_i] = volume_batch_to_aggregate.max(0)[0]\n",
    "        elif volume_aggregation_method == 'softmax':\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate.clone()\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate_softmin.view(n_views, -1)\n",
    "            volume_batch_to_aggregate_softmin = nn.functional.softmax(volume_batch_to_aggregate_softmin, dim=0)\n",
    "            volume_batch_to_aggregate_softmin = volume_batch_to_aggregate_softmin.view(n_views, n_joints, *volume_shape)\n",
    "\n",
    "            volume_batch[batch_i] = (volume_batch_to_aggregate * volume_batch_to_aggregate_softmin).sum(0)\n",
    "\n",
    "        elif volume_aggregation_method == 'no_aggregation':\n",
    "            volume_batch.append(volume_batch_to_aggregate.view(-1, *volume_shape)*(timestep_multiplier if view_i != n_views//2 else 1.0)) \n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown volume_aggregation_method: {}\".format(volume_aggregation_method))\n",
    "               \n",
    "    return volume_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shorted =  batch.copy()\n",
    "batch_shorted['keypoints_3d'] = batch_shorted['keypoints_3d'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EasyDict({'training': True,\n",
    "                  'rotation':False,\n",
    "                  'kind':'mpii',\n",
    "                  'transfer_cmu_to_human36m':False,\n",
    "                  'use_precalculated_pelvis': False,\n",
    "                  'use_gt_pelvis': True,\n",
    "                  'use_separate_v2v_for_basepoint': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_t = []\n",
    "grid_old_t = []\n",
    "sizes = list(range(1,40))\n",
    "\n",
    "for i in tqdm_notebook(sizes):\n",
    "    \n",
    "    keypoints = np.random.randn(i,17,4)\n",
    "    batch_shorted['keypoints_3d'] = keypoints\n",
    "    \n",
    "    \n",
    "    t1 = time()\n",
    "    grid = get_coord_volumes(model, \n",
    "                            cuboid_side, \n",
    "                            volume_size, \n",
    "                            device, \n",
    "                            keypoints = keypoints)\n",
    "\n",
    "    t2 = time()\n",
    "    grid_old = get_coord_volumes_old(model, \n",
    "                                    batch_shorted, \n",
    "                                    cuboid_side, \n",
    "                                    volume_size, \n",
    "                                    device, \n",
    "                                    determine_base_point = True, \n",
    "                                    tri_keypoints_3d = None\n",
    "                                    )\n",
    "\n",
    "    t3 = time()\n",
    "    \n",
    "    grid_t += [round(t2-t1,4)]\n",
    "    grid_old_t += [round(t3-t2,4)]\n",
    "    \n",
    "    assert bool(torch.all(grid == grid_old[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks = sizes\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(grid_old_t, label='old_coord_volume_generator')\n",
    "plt.plot(grid_t, 'r', label='new_coord_volume_generator')\n",
    "plt.xticks(ticks = range(len(xticks)), labels=xticks)\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('time, sec.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
