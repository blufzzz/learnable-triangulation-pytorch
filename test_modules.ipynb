{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from mvn.models.triangulation import  VolumetricTriangulationNet\n",
    "from mvn.models.volumetric_adain import VolumetricTemporalAdaINNet\n",
    "from mvn.models.v2v import AdaIN, V2VModel, Basic3DBlock, Res3DBlock, Upsample3DBlock, Pool3DBlock, EncoderDecorder, C3D\n",
    "from mvn.models import v2v\n",
    "from mvn.models.temporal import get_normalization, Res1DBlock\n",
    "from mvn.models.loss import KeypointsMSELoss, KeypointsMSESmoothLoss, KeypointsMAELoss, KeypointsL2Loss, VolumetricCELoss\n",
    "from mvn.models import pose_hrnet, pose_resnet\n",
    "from mvn.utils import img, multiview, op, vis, misc, cfg\n",
    "from mvn.datasets.human36m import Human36MTemporalDataset, Human36MMultiViewDataset\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "from mvn.utils import img\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils import op\n",
    "from mvn.utils import vis\n",
    "from mvn.utils import misc\n",
    "from mvn.utils import cfg\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image,to_numpy\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from pytorch_convolutional_rnn.convolutional_rnn import Conv3dLSTM, Conv3dPeepholeLSTM, Conv2dLSTMCell, Conv2dPeepholeLSTM\n",
    "\n",
    "CONNECTIVITY_DICT = {\n",
    "    'cmu': [(0, 2), (0, 9), (1, 0), (1, 17), (2, 12), (3, 0), (4, 3), (5, 4), (6, 2), (7, 6), (8, 7), (9, 10), (10, 11), (12, 13), (13, 14), (15, 1), (16, 15), (17, 18)],\n",
    "    'coco': [(0, 1), (0, 2), (1, 3), (2, 4), (5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16), (5, 6), (5, 11), (6, 12), (11, 12)],\n",
    "    \"mpii\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 9), (8, 12), (8, 13), (10, 11), (11, 12), (13, 14), (14, 15)],\n",
    "    \"human36m\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 16), (9, 16), (8, 12), (11, 12), (10, 11), (8, 13), (13, 14), (14, 15)],\n",
    "    \"kth\": [(0, 1), (1, 2), (5, 4), (4, 3), (6, 7), (7, 8), (11, 10), (10, 9), (2, 3), (3, 9), (2, 8), (9, 12), (8, 12), (12, 13)],\n",
    "}\n",
    "\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "def get_capacity(model):\n",
    "    s_total = 0\n",
    "    for param in model.parameters():\n",
    "        s_total+=param.numel()\n",
    "    return round(s_total / (10**6),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDecoderLSTM(nn.Module):\n",
    "    def __init__(self, style_vector_dim, feature_space, hidden_dim=512, time=5):\n",
    "        super(FeatureDecoderLSTM, self).__init__()\n",
    "        self.style_vector_dim = style_vector_dim\n",
    "        self.feature_space = feature_space\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.style2cell_state = nn.Sequential(nn.ConvTranspose3d(style_vector_dim, hidden_dim//4,kernel_size=[1,2,2], stride=[1,2,2]),\n",
    "                                             nn.LeakyReLU(), \n",
    "                                             nn.GroupNorm(32, hidden_dim//4),\n",
    "                                             nn.ConvTranspose3d(hidden_dim//4,hidden_dim//2,kernel_size=[1,2,2], stride=[1,2,2]),\n",
    "                                             nn.LeakyReLU(), \n",
    "                                             nn.GroupNorm(32, hidden_dim//2),\n",
    "                                             nn.Conv3d(hidden_dim//2,hidden_dim,kernel_size=[time,1,1], stride=1),\n",
    "                                             nn.LeakyReLU()\n",
    "                                             )\n",
    "        \n",
    "        self.lstm_cell = Conv2dLSTMCell(in_channels=feature_space,\n",
    "                                       out_channels=hidden_dim,\n",
    "                                       kernel_size=feature_space,\n",
    "                                       bias=True)\n",
    "        \n",
    "        self.hidden2feature = nn.Conv2d(hidden_dim, feature_space, kernel_size=1)\n",
    "\n",
    "    def forward(self, style_vector, init_feature_map, time=5):\n",
    "        \n",
    "        hx_init = torch.zeros(1, self.hidden_dim, 96, 96).cuda()\n",
    "        cx_init = self.style2cell_state(style_vector).squeeze(2)\n",
    "        output = []\n",
    "        for i in range(time):\n",
    "            if i == 0:\n",
    "                hx, cx = cell(init_feature_map, (hx_init, cx_init))\n",
    "            else:\n",
    "                hx, cx = cell(feature, (hx, cx))\n",
    "                feature = self.hidden2feature(hx)\n",
    "            output.append(feature)    \n",
    "        \n",
    "        output = torch.stack(output,1)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  8 23:42:25 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.25                 Driver Version: 390.25                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 51%   57C    P8    18W / 280W |   2375MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 79%   79C    P2   154W / 280W |   9161MiB / 11178MiB |     91%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     22148      C   ...ibulygin/miniconda3/envs/mvn/bin/python   505MiB |\n",
      "|    0     30686      C   ...ibulygin/miniconda3/envs/mvn/bin/python  1851MiB |\n",
      "|    1     16638      C   ...lygin/miniconda3/envs/mvn/bin/python3.6  9149MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeatureDecoderLSTM(64,64, hidden_dim=64)#.cuda()\n",
    "# model(s, torch.randn(1,256,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 1, 2, 2])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 32, 1, 2, 2])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 64, 64, 64])\n",
      "torch.Size([256, 64, 64, 64])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([64, 64, 1, 1])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for m in model.parameters():\n",
    "    print (m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_capacity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cell = Conv2dLSTMCell(in_channels=3, out_channels=5, kernel_size=3).cuda()\n",
    "time = 6\n",
    "input = torch.randn(time, 16, 3, 10, 10).cuda()\n",
    "output = []\n",
    "for i in range(time):\n",
    "    if i == 0:\n",
    "        hx, cx = cell(input[i])\n",
    "    else:\n",
    "        hx, cx = cell(input[i], (hx, cx))\n",
    "    output.append(hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All model sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 \n",
      " dt: 11 \n",
      " dilation: 3 \n",
      " pivot_type: intermediate \n",
      " pivot_position: 5 \n",
      " keypoints_per_frame False\n",
      "Loading pretrained weights from: ./data/pose_resnet_4.5_pixels_human36m.pth\n",
      "Successfully loaded pretrained weights for backbone\n",
      "Only resnet50 backbone is used...\n",
      "backbone:  34.0M params\n",
      "features_sequence_to_vector:  5.47M params\n",
      "encoder:  0.0M params\n",
      "volume_net:  57.39M params\n",
      "process_features:  0.02M params\n",
      "style_decoder:  2.11M params\n",
      "Loaded vol_temporal_adain model\n"
     ]
    }
   ],
   "source": [
    "config_path = './experiments/human36m/train/human36m_vol_temporal_adain.yaml'\n",
    "\n",
    "config = cfg.load_config(config_path)\n",
    "config.dataset.val.retain_every_n_frames_in_test = 1\n",
    "\n",
    "_, val_loader, _ = setup_human36m_dataloaders(config,\n",
    "                                             is_train=False,\n",
    "                                             distributed_train=False)\n",
    "\n",
    "batch_size, dt, dilation = val_loader.batch_size, val_loader.dataset.dt, val_loader.dataset.dilation\n",
    "\n",
    "pivot_type = config.dataset.pivot_type \n",
    "keypoints_per_frame = config.dataset.val.keypoints_per_frame if hasattr(config.dataset.val, 'keypoints_per_frame') else False\n",
    "pivot_position = {'first':-1, 'intermediate':dt//2}[pivot_type]\n",
    "\n",
    "if not hasattr(config.model.backbone, 'group_norm'):\n",
    "    config.model.backbone.group_norm = False\n",
    "\n",
    "print('Batch size:', batch_size, '\\n',\\\n",
    "      'dt:', dt, '\\n',\\\n",
    "      'dilation:', dilation, '\\n',\\\n",
    "      'pivot_type:', pivot_type, '\\n',\\\n",
    "      'pivot_position:', pivot_position,'\\n',\\\n",
    "      'keypoints_per_frame', keypoints_per_frame)\n",
    "    \n",
    "model = {\n",
    "    \"vol\": VolumetricTriangulationNet,\n",
    "    \"vol_temporal_adain\":VolumetricTemporalAdaINNet\n",
    "}[config.model.name](config, device=device).to(device)\n",
    "\n",
    "print ('Loaded {} model'.format(config.model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Indices for islice() must be None or an integer: 0 <= x <= sys.maxsize.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3355242b5e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mn_views\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Indices for islice() must be None or an integer: 0 <= x <= sys.maxsize."
     ]
    }
   ],
   "source": [
    "for batch in islice(50,val_loader, 3):\n",
    "\n",
    "    n_views = batch['images'].shape[1]\n",
    "    batch_size = batch['images'].shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=n_views, nrows=1, figsize=(5 * n_views, 5))\n",
    "    for i in range(n_views):\n",
    "        # first row\n",
    "        detection = batch['detections'][0][i]\n",
    "        *bbox, c = detection\n",
    "        \n",
    "        image = batch['images'][0][i]\n",
    "        image = denormalize_image(image).astype(np.uint8)\n",
    "        image = image[..., ::-1]  # bgr -> rgb\n",
    "        camera_name = batch['cameras'][i][0].name\n",
    "        title = \"{}, detection conf: {:.3}\".format(camera_name, c)\n",
    "        \n",
    "        # second row\n",
    "        keypoints_3d = batch['keypoints_3d'][0][:, :3]\n",
    "        proj_matrix = batch['cameras'][i][0].projection\n",
    "#         image_shape_before_resize = batch['image_shapes_before_resize'][0][i]\n",
    "        image_shape = image.shape[:2]\n",
    "\n",
    "        keypoints_2d_wrt_new = multiview.project_3d_points_to_image_plane_without_distortion(proj_matrix, keypoints_3d)\n",
    "#         keypoints_2d_wrt_orig = transform_points_after_crop_and_resize(keypoints_2d_wrt_new, (bbox[0], bbox[1]), image_shape_before_resize, image_shape, forward=True)\n",
    "        \n",
    "        axes[i].set_xlim(0, image.shape[1])\n",
    "        axes[i].set_ylim(0, image.shape[0])\n",
    "        axes[i].invert_yaxis()\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].scatter(keypoints_2d_wrt_new[:, 0], keypoints_2d_wrt_new[:, 1], s=10, c='red')\n",
    "        \n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 5, 24, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.randn(1,256,5,96,96).cuda()\n",
    "s = model.features_sequence_to_vector(f)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = val_loader.dataset.labels['table'][200]['keypoints']\n",
    "keypoints = keypoints[:,[1,2]]\n",
    "connectivity = CONNECTIVITY_DICT['human36m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "# points\n",
    "point_size=2\n",
    "ax.scatter(keypoints[:, 0], keypoints[:, 1], c='red', s=point_size)\n",
    "line_width=1\n",
    "radius=None\n",
    "color=None\n",
    "\n",
    "keypoints_mask = [True] * len(keypoints)\n",
    "# connections\n",
    "for (index_from, index_to) in connectivity:\n",
    "    if keypoints_mask[index_from] and keypoints_mask[index_to]:\n",
    "        xs, ys = [np.array([keypoints[index_from, j], keypoints[index_to, j]]) for j in range(2)]\n",
    "        ax.plot(xs, ys, c=color, lw=line_width)\n",
    "\n",
    "if radius is not None:\n",
    "    root_keypoint_index = 0\n",
    "    xroot, yroot = keypoints[root_keypoint_index, 0], keypoints[root_keypoint_index, 1]\n",
    "\n",
    "    ax.set_xlim([-radius + xroot, radius + xroot])\n",
    "    ax.set_ylim([-radius + yroot, radius + yroot])\n",
    "\n",
    "ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained R(2+1)D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 2, 14, 14])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_extractor = VideoResNet(block=BasicBlock,\n",
    "                                conv_makers=[Conv2Plus1D] * 4,\n",
    "                                layers=[3, 4, 6, 3], # [3,4,6,3]\n",
    "                                stem=R2Plus1dStem)\n",
    "model.layer2[0].conv2[0] = Conv2Plus1D(128, 128, 288)\n",
    "model.layer3[0].conv2[0] = Conv2Plus1D(256, 256, 576)\n",
    "model.layer4[0].conv2[0] = Conv2Plus1D(512, 512, 1152)\n",
    "\n",
    "x = torch.randn(3,3,8,112,112)\n",
    "new_model = nn.Sequential(motion_extractor.stem, \n",
    "                          motion_extractor.layer1, \n",
    "                          motion_extractor.layer2,\n",
    "                          motion_extractor.layer3)\n",
    "new_model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n =nn.Sequential()\n",
    "n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.97, 63.74)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_capacity(new_model), get_capacity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video.resnet import VideoResNet, BasicBlock, R2Plus1dStem, Conv2Plus1D\n",
    "weights_path = './data/r2plus1d_34_clip8_ig65m_from_scratch-9bae36ae.pth'\n",
    "\n",
    "motion_extractor = VideoResNet(block=BasicBlock,\n",
    "                                conv_makers=[Conv2Plus1D] * 4,\n",
    "                                layers= [3,4,6,3], \n",
    "                                stem=R2Plus1dStem)\n",
    "model.layer2[0].conv2[0] = Conv2Plus1D(128, 128, 288)\n",
    "model.layer3[0].conv2[0] = Conv2Plus1D(256, 256, 576)\n",
    "model.layer4[0].conv2[0] = Conv2Plus1D(512, 512, 1152)\n",
    "\n",
    "weights_dict = torch.load(motion_extractor_path, map_location=device)\n",
    "model_dict = motion_extractor.state_dict()\n",
    "new_pretrained_state_dict = {}\n",
    "\n",
    "for k, v in weights_dict.items():\n",
    "    if k in model_dict:\n",
    "        new_pretrained_state_dict[k] = weights_dict[k]\n",
    "    else:    \n",
    "        print (k, 'hasnt been loaded in C3D')\n",
    "\n",
    "# We need exact Caffe2 momentum for BatchNorm scaling\n",
    "if normalization_type == 'batch_norm':\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm3d):\n",
    "            m.eps = 1e-3\n",
    "            m.momentum = 0.9        \n",
    "        \n",
    "motion_extractor.load_state_dict(new_pretrained_state_dict)\n",
    "get_capacity(motion_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.74"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_capacity(motion_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained V2V: C3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4a.weight hasnt been loaded in C3D\n",
      "conv4a.bias hasnt been loaded in C3D\n",
      "conv4b.weight hasnt been loaded in C3D\n",
      "conv4b.bias hasnt been loaded in C3D\n",
      "conv5a.weight hasnt been loaded in C3D\n",
      "conv5a.bias hasnt been loaded in C3D\n",
      "conv5b.weight hasnt been loaded in C3D\n",
      "conv5b.bias hasnt been loaded in C3D\n",
      "fc6.weight hasnt been loaded in C3D\n",
      "fc6.bias hasnt been loaded in C3D\n",
      "fc7.weight hasnt been loaded in C3D\n",
      "fc7.bias hasnt been loaded in C3D\n",
      "fc8.weight hasnt been loaded in C3D\n",
      "fc8.bias hasnt been loaded in C3D\n",
      "2.88\n"
     ]
    }
   ],
   "source": [
    "features_sequence_to_vector = C3D(5,3)    \n",
    "\n",
    "weights_dict = torch.load('./data/c3d.pickle', map_location=device)\n",
    "model_dict = features_sequence_to_vector.state_dict()\n",
    "new_pretrained_state_dict = {}\n",
    "\n",
    "for k, v in weights_dict.items():\n",
    "    if k in model_dict:\n",
    "        new_pretrained_state_dict[k] = weights_dict[k]\n",
    "    else:    \n",
    "        print (k, 'hasnt been loaded in C3D')\n",
    "\n",
    "features_sequence_to_vector.load_state_dict(new_pretrained_state_dict)\n",
    "features_sequence_to_vector.to(device)\n",
    "print (get_capacity(features_sequence_to_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TemporalDiscr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDiscriminator(nn.Module):\n",
    "    \"\"\"docstring for TemporalDiscriminator\"\"\"\n",
    "    def __init__(self, \n",
    "                 input_features_dim, \n",
    "                 output_features_dim=1024, \n",
    "                 intermediate_channels=512, \n",
    "                 normalization_type='group_norm',\n",
    "                 dt = 8,\n",
    "                 kernel_size = 3,\n",
    "                 n_groups = 32):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_features_dim = input_features_dim\n",
    "        self.output_features_dim = output_features_dim\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.normalization_type = normalization_type\n",
    "        \n",
    "        self.first_block = Res1DBlock(input_features_dim, \n",
    "                                      intermediate_channels,\n",
    "                                      kernel_size=1,\n",
    "                                      normalization_type=normalization_type)\n",
    "        \n",
    "        l = dt\n",
    "        blocks =  []\n",
    "        while l >= kernel_size:\n",
    "            l = l - kernel_size + 1\n",
    "            blocks.append(Res1DBlock(intermediate_channels, \n",
    "                                     intermediate_channels, \n",
    "                                     kernel_size=kernel_size,\n",
    "                                     normalization_type=normalization_type))\n",
    "\n",
    "            blocks.append(nn.Sequential(nn.Conv1d(intermediate_channels, intermediate_channels, kernel_size=3, padding=1),\n",
    "                                        get_normalization(normalization_type, intermediate_channels, n_groups=n_groups, dimension=1),\n",
    "                                        nn.ReLU(True),\n",
    "                                        nn.Conv1d(intermediate_channels, intermediate_channels, kernel_size=3, padding=1),\n",
    "                                        get_normalization(normalization_type, intermediate_channels, n_groups=n_groups, dimension=1),\n",
    "                                        nn.ReLU(True)\n",
    "                                        ))\n",
    "        \n",
    "        self.blocks = nn.Sequential(*blocks)    \n",
    "        self.final_block = nn.Conv1d(intermediate_channels, \n",
    "                                      output_features_dim,\n",
    "                                      kernel_size=l)\n",
    "        \n",
    "    def forward(self, x, device='cuda:0'):\n",
    "        # [batch_size, dt, feature_shape]\n",
    "        x = x.transpose(1,2) # [batch_size, dt, feature_shape] -> [batch_size, feature_shape, dt]\n",
    "        x  = self.first_block(x)\n",
    "        x  = self.blocks(x)\n",
    "        x  = self.final_block(x)\n",
    "        \n",
    "        return x[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,8,17,3)\n",
    "[1,2,3,4][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
