{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MTemporalDataset\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image,to_numpy, \\\n",
    "                          get_square_bbox, resize_image, crop_image, normalize_image, scale_bbox\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from mvn.utils.multiview import Camera, project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "from mvn.utils import op\n",
    "from mvn.utils import cfg\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "%matplotlib inline\n",
    "\n",
    "retval = {\n",
    "    'subject_names': ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11'],\n",
    "    'camera_names': ['54138969', '55011271', '58860488', '60457274'],\n",
    "    'action_names': [\n",
    "        'Directions-1', 'Directions-2',\n",
    "        'Discussion-1', 'Discussion-2',\n",
    "        'Eating-1', 'Eating-2',\n",
    "        'Greeting-1', 'Greeting-2',\n",
    "        'Phoning-1', 'Phoning-2',\n",
    "        'Posing-1', 'Posing-2',\n",
    "        'Purchases-1', 'Purchases-2',\n",
    "        'Sitting-1', 'Sitting-2',\n",
    "        'SittingDown-1', 'SittingDown-2',\n",
    "        'Smoking-1', 'Smoking-2',\n",
    "        'TakingPhoto-1', 'TakingPhoto-2',\n",
    "        'Waiting-1', 'Waiting-2',\n",
    "        'Walking-1', 'Walking-2',\n",
    "        'WalkingDog-1', 'WalkingDog-2',\n",
    "        'WalkingTogether-1', 'WalkingTogether-2']\n",
    "}\n",
    "\n",
    "JOINT_H36_DICT = {0:'RFoot',\n",
    "                 1:'RKnee',\n",
    "                 2:'RHip',\n",
    "                 3:'LHip',\n",
    "                 4:'LKnee',\n",
    "                 5:'LFoot',\n",
    "                 6:'Hip',\n",
    "                 7:'Spine',\n",
    "                 8:'Thorax',\n",
    "                 9:'Head',\n",
    "                 10:'RWrist',\n",
    "                 11:'RElbow',\n",
    "                 12:'RShoulder',\n",
    "                 13:'LShoulder',\n",
    "                 14:'LElbow',\n",
    "                 15:'LWrist',\n",
    "                 16:'Neck/Nose'}\n",
    "\n",
    "CONNECTIVITY_DICT = {\n",
    "    'cmu': [(0, 2), (0, 9), (1, 0), (1, 17), (2, 12), (3, 0), (4, 3), (5, 4), (6, 2), (7, 6), (8, 7), (9, 10), (10, 11), (12, 13), (13, 14), (15, 1), (16, 15), (17, 18)],\n",
    "    'coco': [(0, 1), (0, 2), (1, 3), (2, 4), (5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16), (5, 6), (5, 11), (6, 12), (11, 12)],\n",
    "    \"mpii\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 9), (8, 12), (8, 13), (10, 11), (11, 12), (13, 14), (14, 15)],\n",
    "    \"human36m\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 16), (9, 16), (8, 12), (11, 12), (10, 11), (8, 13), (13, 14), (14, 15)],\n",
    "    \"kth\": [(0, 1), (1, 2), (5, 4), (4, 3), (6, 7), (7, 8), (11, 10), (10, 9), (2, 3), (3, 9), (2, 8), (9, 12), (8, 12), (12, 13)],\n",
    "}\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By default, there are 159 181 frames in training set and 26 634 in test (val) set.  \n",
    "With this parameter, test set frames will be evenly skipped frames so that the  \n",
    "test set size is `26634 // retain_every_n_frames_test`.  \n",
    "Use a value of 13 to get 2049 frames in test set.  \n",
    "with_damaged_actions:  \n",
    "If `True`, will include 'S9/[Greeting-2,SittingDown-2,Waiting-1]' in test set.  \n",
    "kind:  \n",
    "Keypoint format, 'mpii' or 'human36m'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 \n",
      " dt: 12 \n",
      " dilation: 3 \n",
      " pivot_type: first \n",
      " pivot_position: -1 \n",
      " keypoints_per_frame False\n"
     ]
    }
   ],
   "source": [
    "experiment_type = 'resnet_50/adain_1d'\n",
    "experiment_name = 'h36_sv32_dist_adain-all-gn_s2v-lstm-1024-1024-1024-gn_vf32_f2v-backbone-C4-1024-group_resnet50-gn-nostylegrad_dt-12_dil-3-1-1_lr-1e-4@12.03.2020-19:00:21'     \n",
    "\n",
    "experiment_root = os.path.join('../logs/', experiment_type, experiment_name)\n",
    "config_path = experiment_root + '/config.yaml'\n",
    "\n",
    "config = cfg.load_config(config_path)\n",
    "\n",
    "train_loader, val_loader, _ = setup_human36m_dataloaders(config,\n",
    "                                             is_train=True,\n",
    "                                             distributed_train=False)\n",
    "\n",
    "batch_size, dt, dilation = val_loader.batch_size, val_loader.dataset.dt, val_loader.dataset.dilation\n",
    "\n",
    "pivot_type = config.dataset.pivot_type \n",
    "keypoints_per_frame = config.dataset.val.keypoints_per_frame if hasattr(config.dataset.val, 'keypoints_per_frame') else False\n",
    "pivot_position = {'first':-1, 'intermediate':dt//2}[pivot_type]\n",
    "\n",
    "if not hasattr(config.model.backbone, 'group_norm'):\n",
    "    config.model.backbone.group_norm = False\n",
    "\n",
    "print('Batch size:', batch_size, '\\n',\\\n",
    "      'dt:', dt, '\\n',\\\n",
    "      'dilation:', dilation, '\\n',\\\n",
    "      'pivot_type:', pivot_type, '\\n',\\\n",
    "      'pivot_position:', pivot_position,'\\n',\\\n",
    "      'keypoints_per_frame', keypoints_per_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_loader.dataset\n",
    "train_dataset = train_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject_names', 'camera_names', 'action_names', 'cameras', 'table'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in islice(val_loader, 3):\n",
    "\n",
    "    n_views = batch['images'].shape[1]\n",
    "    batch_size = batch['images'].shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=n_views, nrows=1, figsize=(5 * n_views, 5))\n",
    "    for i in range(n_views):\n",
    "        # first row\n",
    "        detection = batch['detections'][0][i]\n",
    "        *bbox, c = detection\n",
    "        \n",
    "        image = batch['images'][0][i]\n",
    "        image = denormalize_image(image).astype(np.uint8)\n",
    "        image = image[..., ::-1]  # bgr -> rgb\n",
    "        camera_name = batch['cameras'][i][0].name\n",
    "        title = \"{}, detection conf: {:.3}\".format(camera_name, c)\n",
    "        \n",
    "        # second row\n",
    "        keypoints_3d = batch['keypoints_3d'][0][:, :3]\n",
    "        proj_matrix = batch['cameras'][i][0].projection\n",
    "#         image_shape_before_resize = batch['image_shapes_before_resize'][0][i]\n",
    "        image_shape = image.shape[:2]\n",
    "\n",
    "        keypoints_2d_wrt_new = multiview.project_3d_points_to_image_plane_without_distortion(proj_matrix, keypoints_3d)\n",
    "#         keypoints_2d_wrt_orig = transform_points_after_crop_and_resize(keypoints_2d_wrt_new, (bbox[0], bbox[1]), image_shape_before_resize, image_shape, forward=True)\n",
    "        \n",
    "        axes[i].set_xlim(0, image.shape[1])\n",
    "        axes[i].set_ylim(0, image.shape[0])\n",
    "        axes[i].invert_yaxis()\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].scatter(keypoints_2d_wrt_new[:, 0], keypoints_2d_wrt_new[:, 1], s=10, c='red')\n",
    "        \n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
