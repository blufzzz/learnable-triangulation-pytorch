{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from mvn.models.triangulation import RANSACTriangulationNet, AlgebraicTriangulationNet, VolumetricTriangulationNet\n",
    "from mvn.models.volumetric_temporal import VolumetricTemporalNet,\\\n",
    "                                           VolumetricTemporalAdaINNet,\\\n",
    "                                           VolumetricFRAdaINNet\n",
    "from mvn.models.v2v import AdaIN, V2VModel, Basic3DBlock, Res3DBlock, Upsample3DBlock, Pool3DBlock, EncoderDecorder\n",
    "from mvn.models.loss import KeypointsMSELoss, KeypointsMSESmoothLoss, KeypointsMAELoss, KeypointsL2Loss, VolumetricCELoss\n",
    "from mvn.utils import img, multiview, op, vis, misc, cfg\n",
    "from mvn.datasets.human36m import Human36MTemporalDataset, Human36MMultiViewDataset\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "from mvn.utils import img\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils import op\n",
    "from mvn.utils import vis\n",
    "from mvn.utils import misc\n",
    "from mvn.utils import cfg\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image,to_numpy\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from pytorch_convolutional_rnn.convolutional_rnn import Conv2dLSTM, Conv2dPeepholeLSTM\n",
    "\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "\n",
    "def get_n(m):\n",
    "    s = 0\n",
    "    for param in m.parameters():\n",
    "        s+=param.nelement()\n",
    "    print (m.__class__, s//1024)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = './experiments/human36m/train/human36m_backbone.yaml'\n",
    "config = cfg.load_config(config_path)\n",
    "train_loader, val_loader, _ = setup_human36m_dataloaders(config,\n",
    "                                                         is_train=True,\n",
    "                                                         distributed_train=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "    \n",
    "images_batch, keypoints_3d_batch_gt, keypoints_3d_validity_batch_gt, proj_matricies_batch = dataset_utils.prepare_batch(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid, cuboids, base_points = op.get_coord_volumes(kind='mpii', \n",
    "                                                    training=True, \n",
    "                                                    rotation=False,\n",
    "                                                    cuboid_side=2500, \n",
    "                                                    volume_size=64, \n",
    "                                                    device=device, \n",
    "                                                    keypoints = keypoints_3d_batch_gt,\n",
    "                                                    batch_size = None,\n",
    "                                                    dt = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([241.5050,   1.1129, 729.7400], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index= 0\n",
    "grid[batch_index,-1,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid[batch_index,-1,-1,-1] - grid[batch_index,-1,-1,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
