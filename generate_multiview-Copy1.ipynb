{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from tqdm import tqdm_notebook\n",
    "from time import time\n",
    "from easydict import EasyDict\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mvn.utils.img import image_batch_to_numpy, denormalize_image,to_numpy\n",
    "from mvn.models.pose_hrnet import get_pose_net as get_pose_hrnet\n",
    "from mvn.models.pose_resnet import get_pose_net as get_pose_resnet\n",
    "\n",
    "from mvn.utils.multiview import project_3d_points_to_image_plane_without_distortion\n",
    "from mvn.utils.vis import draw_2d_pose\n",
    "from mvn.utils import img\n",
    "from mvn.utils import multiview\n",
    "from mvn.utils import volumetric\n",
    "from mvn.utils import op\n",
    "from mvn.utils import vis\n",
    "from mvn.utils import misc\n",
    "from mvn.utils import cfg\n",
    "from mvn.datasets import utils as dataset_utils\n",
    "from mvn.datasets.human36m import Human36MMultiViewDataset, Human36MTemporalDataset\n",
    "from mvn.utils.misc import get_start_stop_frame_indxs, index_to_name, get_error_diffs, normalize_temporal_images_batch, retval, get_capacity\n",
    "\n",
    "from train import setup_human36m_dataloaders\n",
    "\n",
    "from mvn.models.triangulation import VolumetricTriangulationNet\n",
    "from mvn.models.volumetric_adain import VolumetricTemporalAdaINNet\n",
    "from mvn.models.volumetric_grid import VolumetricTemporalGridDeformation\n",
    "\n",
    "from mvn.models.v2v import V2VModel\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "retval = {\n",
    "    'subject_names': ['S1', 'S5', 'S6', 'S7', 'S8', 'S9', 'S11'],\n",
    "    'camera_names': ['54138969', '55011271', '58860488', '60457274'],\n",
    "    'action_names': [\n",
    "        'Directions-1', 'Directions-2',\n",
    "        'Discussion-1', 'Discussion-2',\n",
    "        'Eating-1', 'Eating-2',\n",
    "        'Greeting-1', 'Greeting-2',\n",
    "        'Phoning-1', 'Phoning-2',\n",
    "        'Posing-1', 'Posing-2',\n",
    "        'Purchases-1', 'Purchases-2',\n",
    "        'Sitting-1', 'Sitting-2',\n",
    "        'SittingDown-1', 'SittingDown-2',\n",
    "        'Smoking-1', 'Smoking-2',\n",
    "        'TakingPhoto-1', 'TakingPhoto-2',\n",
    "        'Waiting-1', 'Waiting-2',\n",
    "        'Walking-1', 'Walking-2',\n",
    "        'WalkingDog-1', 'WalkingDog-2',\n",
    "        'WalkingTogether-1', 'WalkingTogether-2']\n",
    "}\n",
    "\n",
    "JOINT_H36_DICT = {0:'RFoot',\n",
    "                 1:'RKnee',\n",
    "                 2:'RHip',\n",
    "                 3:'LHip',\n",
    "                 4:'LKnee',\n",
    "                 5:'LFoot',\n",
    "                 6:'Hip',\n",
    "                 7:'Spine',\n",
    "                 8:'Thorax',\n",
    "                 9:'Head',\n",
    "                 10:'RWrist',\n",
    "                 11:'RElbow',\n",
    "                 12:'RShoulder',\n",
    "                 13:'LShoulder',\n",
    "                 14:'LElbow',\n",
    "                 15:'LWrist',\n",
    "                 16:'Neck/Nose'}\n",
    "\n",
    "JOINT_NAMES_DICT = {\n",
    "                    0: \"nose\",\n",
    "                    1: \"left_eye\",\n",
    "                    2: \"right_eye\",\n",
    "                    3: \"left_ear\",\n",
    "                    4: \"right_ear\",\n",
    "                    5: \"left_shoulder\",\n",
    "                    6: \"right_shoulder\",\n",
    "                    7: \"left_elbow\",\n",
    "                    8: \"right_elbow\",\n",
    "                    9: \"left_wrist\",\n",
    "                    10: \"right_wrist\",\n",
    "                    11: \"left_hip\",\n",
    "                    12: \"right_hip\",\n",
    "                    13: \"left_knee\",\n",
    "                    14: \"right_knee\",\n",
    "                    15: \"left_ankle\",\n",
    "                    16: \"right_ankle\"\n",
    "                }\n",
    "\n",
    "CONNECTIVITY_DICT = {\n",
    "    'cmu': [(0, 2), (0, 9), (1, 0), (1, 17), (2, 12), (3, 0), (4, 3), (5, 4), (6, 2), (7, 6), (8, 7), (9, 10), (10, 11), (12, 13), (13, 14), (15, 1), (16, 15), (17, 18)],\n",
    "    'coco': [(0, 1), (0, 2), (1, 3), (2, 4), (5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16), (5, 6), (5, 11), (6, 12), (11, 12)],\n",
    "    \"mpii\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 9), (8, 12), (8, 13), (10, 11), (11, 12), (13, 14), (14, 15)],\n",
    "    \"human36m\": [(0, 1), (1, 2), (2, 6), (5, 4), (4, 3), (3, 6), (6, 7), (7, 8), (8, 16), (9, 16), (8, 12), (11, 12), (10, 11), (8, 13), (13, 14), (14, 15)],\n",
    "    \"kth\": [(0, 1), (1, 2), (5, 4), (4, 3), (6, 7), (7, 8), (11, 10), (10, 9), (2, 3), (3, 9), (2, 8), (9, 12), (8, 12), (12, 13)],\n",
    "}\n",
    "\n",
    "device = 'cuda:0' #torch.cuda.current_device()\n",
    "print ('Done')\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.enabled = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpack tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard.backend.event_processing import event_accumulator\n",
    "# d = {}\n",
    "# exp_path = '../logs/resnet_50'\n",
    "# names = [\n",
    "#     'stack_3d/h36_sv32_dist_stack-pose-noloss-3d-interpolate-gn_s2v-v2v-v1-256-32-gn_vf32_f2v-features-noupscale-C0-256-group_resnet50-gn-nostylegrad_dt-12_dil-3-1-1_lr-1e-4@24.03.2020-22:26:39',\n",
    "#     'baseline/h36_sv32_dist_resnet50-gn_v2v-v1-gn-no-aggr_1-1_lr-1e-4@17.03.2020-15:30:45',\n",
    "#     'baseline/h36_sv32_dist_resnet50-bn_v2v-v1-bn-no-aggr_1-1_lr-1e-4@17.03.2020-15:32:32',\n",
    "#     'stack_3d/h36_sv32_dist_stack3d-interpolate-gn_s2v-v2v-v1-256-128-64-32-gn_vf32_f2v-features-noupscale-C0-256-group_resnet50-gn-nostylegrad_dt-6_dil-3-3-3_lr-1e-4@25.03.2020-17:12:59'\n",
    "# ]\n",
    "# for name in tqdm_notebook(names): #(os.listdir(exp_path))\n",
    "# #     exp_path = os.path.join(root, exp_type)\n",
    "# #     for name in tqdm_notebook(os.listdir(exp_path)):\n",
    "#     path = os.path.join(exp_path, name, 'tb')\n",
    "#     path = os.path.join(path, os.listdir(path)[0])\n",
    "#     ea = event_accumulator.EventAccumulator(path, size_guidance={ # see below regarding this argument\n",
    "#                                             event_accumulator.COMPRESSED_HISTOGRAMS: 0,\n",
    "#                                             event_accumulator.IMAGES: 0,\n",
    "#                                             event_accumulator.AUDIO: 0,\n",
    "#                                             event_accumulator.SCALARS: 100,\n",
    "#                                             event_accumulator.HISTOGRAMS: 0,\n",
    "#                                             })\n",
    "\n",
    "#     ea.Reload()\n",
    "\n",
    "#     if 'val/dataset_metric_epoch' in ea.Tags()['scalars']:\n",
    "#         full_name = name\n",
    "#         dataset_metric_epoch = [event.value for event in ea.Scalars('val/dataset_metric_epoch')]\n",
    "#         d[full_name] = dataset_metric_epoch\n",
    "#     else:\n",
    "#         print (f'Name {full_name} skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # baseline_name_bn = 'baseline/h36_sv32_dist_resnet50-bn_v2v-v1-bn-no-aggr_1-1_lr-1e-4@17.03.2020-15:32:32'\n",
    "# # label1 = 'Baseline Batch Normalization'\n",
    "# plt.figure(figsize=(10,5), dpi=180)\n",
    "# plt.plot(d[names[2]][:16], label = 'Baseline Batch Normalization')\n",
    "# plt.plot(d[names[1]][:16], label = 'Baseline Group Normalization')\n",
    "# plt.plot(d[names[3]][:16], label = 'Stack3D model with Pose-Motion disentanglement')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Validation MPJPE')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # name1 = names[0]\n",
    "# # label1 = 'SPADE3D'\n",
    "\n",
    "# # name2 = names[1]\n",
    "# # label2 = 'STACK3D'\n",
    "\n",
    "# # N = 200\n",
    "\n",
    "# # plt.figure(figsize=(10,5), dpi=180)\n",
    "# # # plt.plot(d[baseline_name_bn][:N], label=label1, linewidth=1.)\n",
    "# # plt.plot(d[name1][:N], label=label1, linewidth=1.)\n",
    "# # plt.plot(d[name2][:N], label=label2, linewidth=1.)\n",
    "# # plt.xlabel('Epoch')\n",
    "# # plt.ylabel('Training Decoder loss (weighted by 1e-4)')\n",
    "# # plt.legend()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_name = 'h36_sv32_dist_baseline_v2v-v1-BN_resnet152-BN_bs-1-1_norot_softmax_15k@12.02.2020-19:52:43'\n",
    "# path = os.path.join(root, special_name, 'tb')\n",
    "# path = os.path.join(path, os.listdir(path)[0])\n",
    "# ea = event_accumulator.EventAccumulator(path, size_guidance={ # see below regarding this argument\n",
    "#                                         event_accumulator.COMPRESSED_HISTOGRAMS: 0,\n",
    "#                                         event_accumulator.IMAGES: 0,\n",
    "#                                         event_accumulator.AUDIO: 0,\n",
    "#                                         event_accumulator.SCALARS: 100,\n",
    "#                                         event_accumulator.HISTOGRAMS: 0,\n",
    "#                                         })\n",
    "\n",
    "# ea.Reload()\n",
    "\n",
    "# l2_1 = [event.value for event in ea.Scalars('val/dataset_metric_epoch')]\n",
    "\n",
    "\n",
    "# special_name = 'h36_sv32_dist_baseline_v2v-v1-BN_resnet152-BN_bs-1-1_norot_softmax_15k@12.02.2020-19:52:43'\n",
    "# path = os.path.join(root, special_name, 'tb')\n",
    "# path = os.path.join(path, os.listdir(path)[0])\n",
    "# ea = event_accumulator.EventAccumulator(path, size_guidance={ # see below regarding this argument\n",
    "#                                         event_accumulator.COMPRESSED_HISTOGRAMS: 0,\n",
    "#                                         event_accumulator.IMAGES: 0,\n",
    "#                                         event_accumulator.AUDIO: 0,\n",
    "#                                         event_accumulator.SCALARS: 100,\n",
    "#                                         event_accumulator.HISTOGRAMS: 0,\n",
    "#                                         })\n",
    "\n",
    "# ea.Reload()\n",
    "\n",
    "# l2_2 = [event.value for event in ea.Scalars('val/dataset_metric_epoch')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1 \n",
      " dt: 11 \n",
      " dilation: 3 \n",
      " pivot_type: intermediate \n",
      " pivot_position: 5 \n",
      " keypoints_per_frame False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# experiment_type='resnet_50_3d'\n",
    "# experiment_name='h36_sv32_dist_spade3d-h128-interpolate-gn-bn-conf-leakyrelu_s2v-R2D-2l-RGB-64-1e-4_vf64_resnet50-gn_dt-6_dil-3-1-1_lr-1e-4_norot_gradclip-1e-2@10.04.2020-17:06:07'\n",
    "\n",
    "experiment_type='resnet_50_3d'\n",
    "experiment_name='h36_sv32_dist_spade3d-128-interpolate-gn-bn-conf-leakyrelu_s2v-v2v-256-128-64-64-gn-decoder-w1e-3-timeweight-exp_vf64_f2v-features-C0-256_resnet50-gn-nostylegrad_dt-11-intermediate_dil-3-1-1_lr-1e-4_norot_gradclip-1e-2@08.04.2020-16:48:58'   \n",
    "\n",
    "# experiment_type='resnet_50_3d'\n",
    "# experiment_name='h36_sv32_dist_stack3d-interpolate-gn_s2v-v2v-256-128-64-32-gn-decoder-1e-4_vf32_f2v-features-noupscale-C0-256-group_resnet50-gn-nostylegrad_dt-11-intermediate_dil-3-1-1_lr-1e-4_norot@03.04.2020-10:37:42'\n",
    "\n",
    "experiment_root = os.path.join('../logs/', experiment_type, experiment_name)\n",
    "config_path = experiment_root + '/config.yaml'\n",
    "\n",
    "config = cfg.load_config(config_path)\n",
    "config.dataset.val.retain_every_n_frames_in_test = 1\n",
    "\n",
    "_, val_loader, _ = setup_human36m_dataloaders(config,\n",
    "                                             is_train=False,\n",
    "                                             distributed_train=False)\n",
    "\n",
    "batch_size, dt, dilation = val_loader.batch_size, val_loader.dataset.dt, val_loader.dataset.dilation\n",
    "\n",
    "pivot_type = config.dataset.pivot_type \n",
    "keypoints_per_frame = config.dataset.keypoints_per_frame if hasattr(config.dataset, 'keypoints_per_frame') else False\n",
    "pivot_position = {'first':-1, 'intermediate':dt//2}[pivot_type]\n",
    "\n",
    "if not hasattr(config.model.backbone, 'group_norm'):\n",
    "    config.model.backbone.group_norm = False\n",
    "\n",
    "print('Batch size:', batch_size, '\\n',\\\n",
    "      'dt:', dt, '\\n',\\\n",
    "      'dilation:', dilation, '\\n',\\\n",
    "      'pivot_type:', pivot_type, '\\n',\\\n",
    "      'pivot_position:', pivot_position,'\\n',\\\n",
    "      'keypoints_per_frame', keypoints_per_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataset output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in islice(val_loader, 3):\n",
    "\n",
    "#     n_views = batch['images'].shape[1]\n",
    "#     batch_size = batch['images'].shape[0]\n",
    "    \n",
    "#     fig, axes = plt.subplots(ncols=n_views, nrows=1, figsize=(5 * n_views, 5))\n",
    "#     for i in range(n_views):\n",
    "#         # first row\n",
    "#         detection = batch['detections'][0][i]\n",
    "#         *bbox, c = detection\n",
    "        \n",
    "#         image = batch['images'][0][i]\n",
    "#         image = denormalize_image(image).astype(np.uint8)\n",
    "#         image = image[..., ::-1]  # bgr -> rgb\n",
    "#         camera_name = batch['cameras'][i][0].name\n",
    "#         title = \"{}, detection conf: {:.3}\".format(camera_name, c)\n",
    "        \n",
    "#         # second row\n",
    "#         keypoints_3d = batch['keypoints_3d'][0][:, :3]\n",
    "#         proj_matrix = batch['cameras'][i][0].projection\n",
    "# #         image_shape_before_resize = batch['image_shapes_before_resize'][0][i]\n",
    "#         image_shape = image.shape[:2]\n",
    "\n",
    "#         keypoints_2d_wrt_new = multiview.project_3d_points_to_image_plane_without_distortion(proj_matrix, keypoints_3d)\n",
    "# #         keypoints_2d_wrt_orig = transform_points_after_crop_and_resize(keypoints_2d_wrt_new, (bbox[0], bbox[1]), image_shape_before_resize, image_shape, forward=True)\n",
    "        \n",
    "#         axes[i].set_xlim(0, image.shape[1])\n",
    "#         axes[i].set_ylim(0, image.shape[0])\n",
    "#         axes[i].invert_yaxis()\n",
    "#         axes[i].imshow(image)\n",
    "#         axes[i].scatter(keypoints_2d_wrt_new[:, 0], keypoints_2d_wrt_new[:, 1], s=10, c='red')\n",
    "        \n",
    "#         axes[i].axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from: ./data/pose_resnet_4.5_pixels_human36m.pth\n",
      "Successfully loaded pretrained weights for backbone\n",
      "Only resnet50 backbone is used...\n",
      "backbone:  34.0M\n",
      "features_sequence_to_vector:  5.47M\n",
      "encoder:  0.0M\n",
      "volume_net:  57.39M\n",
      "process_features:  0.02M\n",
      "style_decoder:  2.11M\n",
      "Loaded vol_temporal_adain model\n"
     ]
    }
   ],
   "source": [
    "# config.model.cuboid_multiplier = 1.0\n",
    "# config.model.rotation = False\n",
    "# config.model.transfer_cmu_to_human36m = False\n",
    "# config.model.encoder_normalization_type = 'group_norm'\n",
    "# config.model.upscale_bottleneck = False\n",
    "# config.model.f2v_intermediate_channels = config.model.intermediate_channels    \n",
    "# config.model.f2v_normalization_type = 'group_norm'   \n",
    "# config.model.v2v_type = 'v1'\n",
    "# config.model.spade_broadcasting_type = 'unprojecting'\n",
    "\n",
    "model = {\n",
    "    \"vol\": VolumetricTriangulationNet,\n",
    "    \"vol_temporal_adain\":VolumetricTemporalAdaINNet\n",
    "}[config.model.name](config, device=device).to(device)\n",
    "\n",
    "print ('Loaded {} model'.format(config.model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me = model.motion_extractor.motion_extractor\n",
    "# ups =  model.motion_extractor.final_layer\n",
    "# me_p = []\n",
    "# for p in me.parameters():\n",
    "#     me_p.append(p.flatten())\n",
    "# me_p = torch.cat(me_p).detach().cpu().numpy()\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.hist(me_p, bins=1000)\n",
    "# plt.yscale('log', nonposy='clip')\n",
    "# plt.show()\n",
    "\n",
    "# ups_p = []\n",
    "# for p in ups.parameters():\n",
    "#     ups_p.append(p.flatten())\n",
    "# ups_p = torch.cat(ups_p).detach().cpu().numpy()\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.hist(ups_p, bins=1000)\n",
    "# plt.yscale('log', nonposy='clip')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "checkpoints_path = experiment_root + '/checkpoints/'\n",
    "weights_path = checkpoints_path + '/weights.pth'\n",
    "weights_dict = torch.load(weights_path)['model_state'] # , map_location=device\n",
    "REBUILD_STATE_DICT = False\n",
    "\n",
    "if REBUILD_STATE_DICT:\n",
    "    model_dict = model.state_dict()\n",
    "    new_pretrained_state_dict = {}\n",
    "\n",
    "    for k, v in weights_dict.items():\n",
    "        if k in model_dict:\n",
    "            new_pretrained_state_dict[k] = weights_dict[k]\n",
    "        \n",
    "    model.load_state_dict(new_pretrained_state_dict, strict=True)\n",
    "else:\n",
    "    model.load_state_dict(weights_dict, strict=True)\n",
    "    \n",
    "print ('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     images_batch, keypoints_3d_gt, keypoints_3d_validity_gt, proj_matricies_batch = dataset_utils.prepare_batch(batch, device)\n",
    "#     break\n",
    "\n",
    "# (keypoints_3d_pred, \n",
    "# features_pred, \n",
    "# volumes_pred, \n",
    "# confidences_pred, \n",
    "# cuboids_pred, \n",
    "# coord_volumes_pred, \n",
    "# base_points_pred) = model(images_batch, batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=val_loader.dataset.labels\n",
    "start_frame_indexes, stop_frame_indxs=get_start_stop_frame_indxs(labels)\n",
    "\n",
    "### define ###\n",
    "length=500\n",
    "action = 'Directions-1'\n",
    "subject = 'S9'\n",
    "camera_index = 0\n",
    "##############\n",
    "\n",
    "action_index=retval['action_names'].index(action)\n",
    "offset = len(retval['action_names']) if subject == 'S11' else 0\n",
    "start=start_frame_indexes[offset:][action_index].item()\n",
    "\n",
    "assert subject in ['S9', 'S11']\n",
    "assert subject, action == index_to_name(start+length, stop_frame_indxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect keypoints from multiview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hpc2_storage/ibulygin/miniconda3/envs/fresh/lib/python3.7/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dc8ae89c884184895b38988218b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hpc2_storage/ibulygin/miniconda3/envs/fresh/lib/python3.7/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STYLE_VECTOR_CONST INITED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/hpc2_storage/ibulygin/learnable-triangulation-pytorch/mvn/models/volumetric_adain.py:399: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  style_vector = torch.tensor(self.STYLE_VECTOR_CONST).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "USE_RANDOM_STYLE_VECTOR = True\n",
    "USE_CONSTANT_STYLE_VECTOR = True\n",
    "RETURN_DECODED_FEATUES = True\n",
    "RETURN_ME_VECTOR = False\n",
    "ADD_IMAGES = True\n",
    "SPL = model.use_style_pose_lstm_loss\n",
    "\n",
    "KIND = model.kind\n",
    "series = defaultdict(list)\n",
    "series['images'] = defaultdict(list)\n",
    "series['proj_matrices'] = defaultdict(list)\n",
    "eval_view = 0 # supported [0,1,2,3] cameras\n",
    "collate_fn = dataset_utils.make_collate_fn(randomize_n_views=False,\n",
    "                                           min_n_views=None,\n",
    "                                           max_n_views=None)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(start,start+length)):\n",
    "        ##############\n",
    "        # EVALUATION #\n",
    "        ##############\n",
    "        batch = val_loader.dataset.__getitem__(i + eval_view*val_loader.dataset.n_sequences)\n",
    "        if batch is None:\n",
    "            'Batch is none...'\n",
    "            break\n",
    "        \n",
    "        batch = collate_fn([batch])\n",
    "        (images_batch, \n",
    "        keypoints_3d_gt, \n",
    "        keypoints_3d_validity_batch_gt, \n",
    "        proj_matricies_batch) = dataset_utils.prepare_batch(batch, device)\n",
    "            \n",
    "        output = model(images_batch, batch, return_me_vector=RETURN_ME_VECTOR)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if USE_RANDOM_STYLE_VECTOR:\n",
    "            randomized_output = model(images_batch, batch, randomize_style=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        if USE_CONSTANT_STYLE_VECTOR:\n",
    "            const_output = model(images_batch, batch, const_style_vector=True)\n",
    "            torch.cuda.empty_cache()    \n",
    "        \n",
    "        if SPL:\n",
    "            for i in range(dt//2):\n",
    "                series[f'gt_future_keypoints_{i}'] = keypoints_3d_gt[:,pivot_position+1+i].detach().cpu().numpy()\n",
    "                series[f'predicted_future_keypoints_{i}'] = output[0][1][:,i].detach().cpu().numpy()\n",
    "            keypoints_3d_gt = keypoints_3d_gt[:,pivot_position]\n",
    "            keypoints_3d_pred = output[0][0]\n",
    "        else:\n",
    "            keypoints_3d_gt = keypoints_3d_gt\n",
    "            keypoints_3d_pred = output[0]\n",
    "        \n",
    "        if RETURN_DECODED_FEATUES:\n",
    "            series['decoded_features'].append(output[-1].detach().cpu().numpy())\n",
    "            series['features_for_loss'].append(output[1].detach().cpu().numpy())\n",
    "        \n",
    "        if RETURN_ME_VECTOR:\n",
    "            series['style_vectors'].append(output[-3][0].detach().cpu().numpy())\n",
    "            series['style_vectors_me'].append(output[-3][1].detach().cpu().numpy())\n",
    "            \n",
    "        else:\n",
    "            series['style_vectors'].append(output[-3].detach().cpu().numpy())\n",
    "        series['unproj_features'].append(output[-2].detach().cpu().numpy())\n",
    "        \n",
    "        keypoints_3d_pred_const_style = const_output[0][0] if SPL else const_output[0]\n",
    "        keypoints_const_style = keypoints_3d_pred_const_style.detach().cpu().numpy()\n",
    "        series['keypoints_const_style'].append(keypoints_const_style)\n",
    "        series['style_vectors_const'].append(const_output[-3].detach().cpu().numpy())\n",
    "        \n",
    "        keypoints_3d_pred_random_style = randomized_output[0][0] if SPL else randomized_output[0]\n",
    "        keypoints_random_style = keypoints_3d_pred_random_style.detach().cpu().numpy()\n",
    "        series['keypoints_random_style'].append(keypoints_random_style)\n",
    "        \n",
    "        batch_size, n_views, n_joints = keypoints_3d_gt.shape[:3]\n",
    "\n",
    "        # normalize all stuff\n",
    "        proj_matricies_batch = proj_matricies_batch[:,pivot_position].detach().cpu().numpy()\n",
    "        images_batch = normalize_temporal_images_batch(images_batch, pivot_position)\n",
    "        keypoints = keypoints_3d_pred.detach().cpu().numpy()\n",
    "        keypoints_gt =  keypoints_3d_gt.detach().cpu().numpy()\n",
    "        \n",
    "        series['keypoints'].append(keypoints)\n",
    "        series['keypoints_gt'].append(keypoints_gt)\n",
    "        \n",
    "        if ADD_IMAGES:\n",
    "            series['images'][eval_view].append(images_batch)\n",
    "            series['proj_matrices'][eval_view].append(proj_matricies_batch)\n",
    "        \n",
    "        ##############\n",
    "        # PROJECTING #\n",
    "        ##############\n",
    "        if ADD_IMAGES:\n",
    "            for view_i in range(4):\n",
    "                if view_i == eval_view:\n",
    "                    continue\n",
    "                else:\n",
    "                    batch = val_loader.dataset.__getitem__(i + view_i*val_loader.dataset.n_sequences)\n",
    "                    batch = collate_fn([batch])\n",
    "\n",
    "                    (images_batch, \n",
    "                    keypoints_3d_gt, \n",
    "                    keypoints_3d_validity_batch_gt, \n",
    "                    proj_matricies_batch) = dataset_utils.prepare_batch(batch, device)\n",
    "\n",
    "                    # normalize all stuff\n",
    "                    proj_matricies_batch = proj_matricies_batch[:,pivot_position].detach().cpu().numpy()\n",
    "                    images_batch = normalize_temporal_images_batch(images_batch, pivot_position)\n",
    "\n",
    "                    series['images'][view_i].append(images_batch)\n",
    "                    series['proj_matrices'][view_i].append(proj_matricies_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in series.keys():\n",
    "    if isinstance(series[k], list):\n",
    "        series[k] = np.concatenate(series[k][:100],0)         \n",
    "#     elif isinstance(series[k], dict):\n",
    "#         for sub_k in series[k].keys():\n",
    "#             series[k][sub_k] = np.concatenate(series[k][sub_k][:100],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in series.keys():\n",
    "#     print (k, series[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shuffle(style_vector):\n",
    "#     idx = torch.randperm(style_vector.nelement())\n",
    "#     return style_vector.view(-1)[idx].view(style_vector.size())\n",
    "# l1= torch.tensor(series['style_vectors'][0])\n",
    "# l2=torch.tensor(series['style_vectors'][0])\n",
    "# torch.norm(l1 -shuffle(l2)).mean()*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(3,3).requires_grad_()\n",
    "# W = torch.randn(3,3).requires_grad_()\n",
    "# y = torch.randn(3,3).requires_grad_()\n",
    "# c = W@x\n",
    "# z = y\n",
    "# # c=c.detach()\n",
    "# s = c + z\n",
    "# (s.norm()+1).backward()\n",
    "# W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b9fa1ebc99d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mflag\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style_vectors_const'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style_vectors_const'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
     ]
    }
   ],
   "source": [
    "# check const-style-vectors\n",
    "if USE_CONSTANT_STYLE_VECTOR:\n",
    "    flag = []\n",
    "    for i in range(length):\n",
    "        for j in range(length): \n",
    "            flag += [(series['style_vectors_const'][j] == series['style_vectors_const'][i]).all()]\n",
    "    print (all(flag)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_vector = torch.tensor(series['style_vectors'][0]).cuda().unsqueeze(0)\n",
    "# next_features = torch.tensor(series['features_for_loss'][0]).cuda().unsqueeze(0)\n",
    "# pred_features = model.style_decoder(style_vector)\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=5, nrows=10)\n",
    "# for i,c in enumerate(np.random.choice(np.arange(256), 10)):\n",
    "#     for j in range(5):\n",
    "#         axes[i,j].imshow(pred_features[0,c,j].detach().cpu().numpy())\n",
    "# fig, axes = plt.subplots(ncols=5, nrows=10)\n",
    "# for i,c in enumerate(np.random.choice(np.arange(256), 10)):\n",
    "#     for j in range(5):\n",
    "#         axes[i,j].imshow(next_features[0,j,c].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=32, nrows=32, figsize=(100,100))\n",
    "# for i in range(32):\n",
    "#     for j in range(32):\n",
    "#         axes[i,j].imshow(series['unproj_features'][0][i,j])\n",
    "#         axes[i,j].set_xticks([])\n",
    "#         axes[i,j].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for joint in range(17):\n",
    "#     diffs_model = []\n",
    "#     diffs_gt = []\n",
    "#     diffs_rand = []\n",
    "    \n",
    "#     for i in range(1, length):\n",
    "#         diffs_model.append(np.linalg.norm(series['keypoints'][i,joint] - np.linalg.norm(series['keypoints'][i-1,joint])))\n",
    "#         diffs_gt.append(np.linalg.norm(series['keypoints_gt'][i,joint] - np.linalg.norm(series['keypoints_gt'][i-1,joint])))\n",
    "#         diffs_rand.append(np.linalg.norm(series['keypoints_random_style'][i,joint] - np.linalg.norm(series['keypoints_random_style'][i-1,joint])))\n",
    "        \n",
    "#     plt.figure()\n",
    "#     plt.plot(diffs_model, label = 'diffs_model')\n",
    "#     plt.plot(diffs_gt, label = 'diffs_gt')\n",
    "#     plt.plot(diffs_rand, label = 'diffs_rand')\n",
    "                          \n",
    "#     plt.title(f'Joint: {joint}')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series['keypoints_gt'] = np.concatenate(series['keypoints_gt'], 0)\n",
    "series['keypoints'] = np.concatenate(series['keypoints'], 0)\n",
    "series['keypoints_const_style'] = np.concatenate(series['keypoints_const_style'], 0)\n",
    "series['keypoints_random_style'] = np.concatenate(series['keypoints_random_style'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series['style_vectors'] = np.concatenate(series['style_vectors'], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series['style_vectors'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPJPE = 82\n",
    "#########\n",
    "# ERROR #   \n",
    "#########\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(20,10))\n",
    "error, gt_diffs, model_diffs = get_error_diffs(series['keypoints_gt'], series['keypoints'])\n",
    "ax1.plot(error, label='Model Error', linewidth=6)\n",
    "if MPJPE is not None:\n",
    "    ax1.plot([MPJPE]*length, '--', color='black', label='Overall MPJPE')\n",
    "\n",
    "title = f\"Error: {error.mean()} \\n\"\n",
    "\n",
    "if USE_CONSTANT_STYLE_VECTOR:\n",
    "    error_const_style, _, model_diffs_const_style = get_error_diffs(series['keypoints_gt'], series['keypoints_const_style'])\n",
    "    ax1.plot(error_const_style, 'red', label='Const Style Error')\n",
    "    title += f\"Error_const_style: {error_const_style.mean()} \\n\"\n",
    "\n",
    "if USE_RANDOM_STYLE_VECTOR:\n",
    "    error_rand_style, _, model_diffs_rand_style = get_error_diffs(series['keypoints_gt'], series['keypoints_random_style'])\n",
    "    ax1.plot(error_rand_style, 'orange', label='Rand Style Error')\n",
    "    title += f\"Error_rand_style: {error_rand_style.mean()} \\n\"\n",
    "\n",
    "ax1.set_xlabel('№ frame')\n",
    "ax1.set_ylabel('MPJPE')\n",
    "ax1.set_title(title)\n",
    "ax1.legend()\n",
    "\n",
    "#########\n",
    "# DIFFS #   \n",
    "#########\n",
    "ax2.plot(model_diffs, label='Model diffs', linewidth=12)\n",
    "\n",
    "if USE_RANDOM_STYLE_VECTOR:\n",
    "    ax2.plot(model_diffs_rand_style, 'orange',label='Model RANDOM_STYLE diffs', linewidth=1)\n",
    "if USE_CONSTANT_STYLE_VECTOR:\n",
    "    ax2.plot(model_diffs_const_style, 'red',label='Model CONST_STYLE diffs')    \n",
    "\n",
    "ax2.plot(gt_diffs, 'g', label='GT diffs')\n",
    "ax2.set_xlabel('№ frame')\n",
    "ax2.set_ylabel(r'$RMSE(keyp_{i}, keyp_{i-1})$')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(error)\n",
    "# plt.plot(error_const_style, 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_vectors = series['style_vectors']#.squeeze(1)\n",
    "# style_vectors_me = series['style_vectors_me']#.squeeze(1)\n",
    "\n",
    "unproj_features = series['unproj_features']\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(style_vectors.flatten(), bins=100)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Number')\n",
    "\n",
    "# plt.show()\n",
    "# plt.close('all')\n",
    "plt.savefig(f\"{os.path.join(experiment_root, 'hist')}\", dpi=280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if series['style_vectors'].ndim == 2:\n",
    "    plt.imshow(series['style_vectors'])\n",
    "    plt.xlabel('feature index')\n",
    "    plt.ylabel(r'$\\leftarrow$№ frame')\n",
    "\n",
    "elif series['style_vectors'].ndim == 4:\n",
    "    n_frames = 12\n",
    "    dilation = 10\n",
    "#     style_channel = np.argmax([style_vectors[:,i,...].reshape(length, -1).ptp(0).max() \\\n",
    "#                                for i in range(style_vectors.shape[-3])])\n",
    "    n_style_channels = style_vectors.shape[-3]\n",
    "    fig, axes = plt.subplots(ncols=n_style_channels,\n",
    "                             nrows=n_frames,\n",
    "                             figsize=(100,100))\n",
    "    for i in range(n_frames):\n",
    "        for j in range(n_style_channels):\n",
    "            t = i*dilation\n",
    "            axes[i,j].imshow(style_vectors[t, j])\n",
    "            axes[i,j].set_xticks([])\n",
    "            axes[i,j].set_yticks([])\n",
    "            if j == 0:\n",
    "                axes[i,j].set_ylabel(f'frame={t}', fontsize=70)\n",
    "    fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "    fig.suptitle('Style-Tensor feature-maps', fontsize=100, y = 0.9)\n",
    "    plt.show()    \n",
    "        \n",
    "elif series['style_vectors'].ndim == 5:\n",
    "    style_channel = np.argmax([style_vectors[:,i,...].reshape(length, -1).ptp(0).max() \\\n",
    "                               for i in range(style_vectors.shape[-3])])\n",
    "    ptp = style_vectors[:, style_channel].reshape(length, -1).ptp(0).max()\n",
    "    n_frames = 12\n",
    "    dilation = 10\n",
    "    n_style_channels = style_vectors.shape[-4]\n",
    "    time_dimension = style_vectors.shape[-3]\n",
    "    n = int(np.sqrt(n_style_channels))\n",
    "    fig, axes = plt.subplots(ncols=time_dimension,\n",
    "                             nrows=n_frames,\n",
    "                             figsize=(100,100))\n",
    "    for i in range(n_frames):    \n",
    "        for j in range(time_dimension):\n",
    "            t = i*dilation\n",
    "            axes[i,j].imshow(style_vectors[t, style_channel, j])\n",
    "            axes[i,j].set_xticks([]) \n",
    "            axes[i,j].set_yticks([])\n",
    "            if j == 0:\n",
    "                axes[i,j].set_ylabel(f'frame={t}', fontsize=70)\n",
    "    fig.subplots_adjust(hspace=0., wspace=0.)\n",
    "    fig.suptitle(f'Style-Tensor slices along first spatial dimension, with channel that gives maximal PTP={np.round(ptp,4)}', fontsize=100, y = 0.9)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp = style_vectors.reshape(length, -1).ptp(0)\n",
    "var = style_vectors.reshape(length, -1).var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ptp)\n",
    "plt.title(f'PTP mean: {ptp.mean()}')\n",
    "plt.ylabel('mean ptp over time')\n",
    "plt.xlabel('feature index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(var)\n",
    "plt.title(f'VAR mean: {var.mean()}')\n",
    "plt.ylabel('mean variance over time')\n",
    "plt.xlabel('feature index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_diffs = []\n",
    "# style_diffs_me = []\n",
    "dilation = 1\n",
    "for i in range(dilation,length):\n",
    "    style_diffs.append(np.linalg.norm(style_vectors[i] - style_vectors[i-dilation]))\n",
    "#     style_diffs_me.append(np.linalg.norm(style_vectors_me[i] - style_vectors_me[i-dilation]))\n",
    "plt.plot(style_diffs) \n",
    "# plt.plot(style_diffs_me, 'r') \n",
    "plt.xlabel('№ frame')\n",
    "plt.ylabel(r'$||s(t_i) - s(t_{i-1}) ||$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_max = range(dilation,length)[np.argmin(style_diffs)] \n",
    "# diff = style_vectors[i_max] - style_vectors[i_max-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrows = 5\n",
    "# fig, axes = plt.subplots(ncols=diff.shape[1],\n",
    "#                              nrows=nrows,\n",
    "#                              figsize=(100,100))\n",
    "# for i in range(nrows):    \n",
    "#     for j in range(diff.shape[1]):\n",
    "#         axes[i,j].imshow(diff[i,j])\n",
    "#         axes[i,j].set_xticks([])\n",
    "#         axes[i,j].set_yticks([])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(ncols=8,\n",
    "#                              nrows=8,\n",
    "#                              figsize=(100,100))\n",
    "\n",
    "# for i in range(8):\n",
    "#     for j in range(8):\n",
    "#         axes[i,j].imshow(w1_flat_dim[:,i*8 + j,...])\n",
    "#         axes[i,j].set_xticks([])\n",
    "#         axes[i,j].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v in d['weight']:\n",
    "#     plt.figure()\n",
    "#     plt.hist(v.flatten(),bins=100)\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import savgol_filter, savgol_coeffs\n",
    "# n_joints = series['keypoints'].shape[1]\n",
    "# series['keypoints_smoothed'] = []\n",
    "# for i in range(n_joints):\n",
    "#     joint_coord_sequence = series['keypoints'][:,i]\n",
    "#     series['keypoints_smoothed'] += [savgol_filter(joint_coord_sequence, window_length=25, polyorder = 2, axis=0)]\n",
    "# series['keypoints_smoothed'] = np.stack(series['keypoints_smoothed'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coord=1\n",
    "# joint = 3\n",
    "# plt.plot(series['keypoints_smoothed'][:,joint,coord][:20],'r', label='smoothed')\n",
    "# plt.plot(series['keypoints'][:,joint,coord][:20], label='original')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = os.path.join('videos/',experiment_type, experiment_name)\n",
    "# if not os.path.isdir(video_path):\n",
    "#     os.makedirs(video_path)\n",
    "#     print ('video_path - Created')\n",
    "# else:\n",
    "#     print ('video_path - Already exists')\n",
    "    \n",
    "# keypoints_dir = os.path.join(video_path,'{0}_{1}'.format(subject, action),'keypoints_videos_lowres')\n",
    "# if not os.path.isdir(keypoints_dir):\n",
    "#     os.makedirs(keypoints_dir)\n",
    "#     print ('keypoints_dir - Created')\n",
    "# else:\n",
    "#     print ('keypoints_dir - Already exists')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_gt = True\n",
    "# with_random_style = True\n",
    "# add_per_joint_description = False\n",
    "# for i in tqdm_notebook(range(1)):\n",
    "#     fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (45,45))\n",
    "#     for view in range(4):\n",
    "\n",
    "#         # unpack\n",
    "#         image = series['images'][view][i]\n",
    "#         proj_matrice = series['proj_matrices'][view][i]\n",
    "#         keypoints_3d_gt = series['keypoints_gt'][i]\n",
    "#         keypoints_3d_pred = series['keypoints'][i] #if not with_random_style else series['keypoints_random_style'][i]\n",
    "\n",
    "#         ax_i = ax.flatten()[view]\n",
    "#         ax_i.imshow(image)\n",
    "\n",
    "#         pjpe = np.linalg.norm(keypoints_3d_gt - keypoints_3d_pred, axis=-1)\n",
    "\n",
    "#         # predicted keypoints\n",
    "#         keypoints_2d_pred_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                                     keypoints_3d_pred)\n",
    "#         draw_2d_pose(keypoints_2d_pred_proj,ax_i,kind='human36m', point_size=200, line_width=5)\n",
    "\n",
    "#         if with_gt:\n",
    "#             keypoints_2d_gt_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                                         keypoints_3d_gt)\n",
    "#             draw_2d_pose(keypoints_2d_gt_proj, ax_i,kind='human36m', point_size=200, line_width=5, color='g')\n",
    "\n",
    "#         if view == eval_view and add_per_joint_description:\n",
    "#             ax_i.set_title(f'EVAL_VIEW, MPJPE: {pjpe.mean()}', fontsize=34)\n",
    "#             text = ''.join([j_name + f': ~{int(pjpe[j_n])}' + '\\n' for j_n,j_name in JOINT_H36_DICT.items()])\n",
    "#             h,w = image.shape[:2]\n",
    "#             offset_1, offset_2 = 5,10\n",
    "#             ax_i.text(0+offset_1, h-offset_2, text, style='italic', fontsize=25,\n",
    "#             bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n",
    "#     plt.show()      \n",
    "# #     plt.savefig('./{}/img_{:05}.jpg'.format(keypoints_dir, i), bbox_inches='tight', dpi=100)\n",
    "# #     plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Normal style winth Random style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints_3d_pred_original\n",
    "# keypoints_3d_ranom_style\n",
    "\n",
    "# plt.fugure()\n",
    "# # predicted keypoints\n",
    "# keypoints_2d_pred_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                             keypoints_3d_pred_original)\n",
    "# draw_2d_pose(keypoints_2d_pred_proj,ax_i,kind='human36m', point_size=200, line_width=5)\n",
    "\n",
    "# plt.fugure()\n",
    "# # predicted keypoints\n",
    "# keypoints_2d_pred_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                             keypoints_3d_ranom_style)\n",
    "# draw_2d_pose(keypoints_2d_pred_proj,ax_i,kind='human36m', point_size=200, line_width=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = os.path.join('videos/','baseline_vs_adain')\n",
    "# if not os.path.isdir(video_path):\n",
    "#     os.makedirs(video_path)\n",
    "#     print ('video_path - Created')\n",
    "# else:\n",
    "#     print ('video_path - Already exists')\n",
    "    \n",
    "# keypoints_dir = os.path.join(video_path,'{0}_{1}'.format(subject, action),'keypoints_videos_lowres')\n",
    "# if not os.path.isdir(keypoints_dir):\n",
    "#     os.makedirs(keypoints_dir)\n",
    "#     print ('keypoints_dir - Created')\n",
    "# else:\n",
    "#     print ('keypoints_dir - Already exists')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_baseline = np.load('action-Directions-1_subj-S9_l100-eval-0_model-vol.npy', allow_pickle=True).item()\n",
    "# series_model = np.load('action-Directions-1_subj-S9_l100-eval-0_model-vol_temporal_adain.npy', allow_pickle=True).item()\n",
    "# add_per_joint_description = False\n",
    "\n",
    "# offset = abs(series_baseline['keypoints_gt'] - series_model['keypoints_gt'][0]).sum(-1).sum(-1).argmin()\n",
    "\n",
    "# for i in tqdm_notebook(range(20)):\n",
    "\n",
    "#     fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (45,45))\n",
    "#     for view in range(4):\n",
    "\n",
    "#         # unpack\n",
    "#         image = series_model['images'][view][:-offset][i]\n",
    "#         proj_matrice = series_model['proj_matrices'][view][:-offset][i]\n",
    "        \n",
    "#         keypoints_3d_baseline = series_baseline['keypoints'][offset:][i]\n",
    "#         keypoints_3d_model = series_model['keypoints'][:-offset][i]\n",
    "        \n",
    "#         keypoints_3d_gt_model = series_model['keypoints_gt'][:-offset][i]\n",
    "#         keypoints_3d_gt_baseline = series_baseline['keypoints_gt'][offset:][i]\n",
    "        \n",
    "#         assert (keypoints_3d_gt_model == keypoints_3d_gt_baseline).all()\n",
    "        \n",
    "#         ax_i = ax.flatten()[view]\n",
    "#         ax_i.imshow(image)\n",
    "\n",
    "#         pjpe_baseline = round(np.linalg.norm(keypoints_3d_gt_baseline - keypoints_3d_baseline, axis=-1).mean(),2)\n",
    "#         pjpe_model = round(np.linalg.norm(keypoints_3d_gt_model - keypoints_3d_model, axis=-1).mean(),2)\n",
    "        \n",
    "#         # predicted keypoints\n",
    "#         keypoints_2d_baseline_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                                          keypoints_3d_baseline)\n",
    "#         draw_2d_pose(keypoints_2d_baseline_proj,ax_i,kind='human36m', point_size=200, line_width=5)\n",
    "\n",
    "#         keypoints_2d_model_proj = project_3d_points_to_image_plane_without_distortion(proj_matrice,\n",
    "#                                                                                     keypoints_3d_model)\n",
    "#         draw_2d_pose(keypoints_2d_model_proj, ax_i,kind='human36m', point_size=200, line_width=5, color='g')\n",
    "    \n",
    "#         ax_i.set_title(f'MPJPE: model-{pjpe_model}, baseline-{pjpe_baseline}', fontsize=34)\n",
    "    \n",
    "    \n",
    "#     plt.savefig('./{}/img_{:05}.jpg'.format(keypoints_dir, i), bbox_inches='tight')\n",
    "#     plt.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
